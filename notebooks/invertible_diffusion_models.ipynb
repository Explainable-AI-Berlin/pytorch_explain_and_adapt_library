{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# TODO adapt these parameters such that they work for your setup\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    x_size = 28  # the generated x resolution\n",
    "    num_channels = 1  # the number of channels in the generated x\n",
    "    train_batch_size = 10\n",
    "    eval_batch_size = 10  # how many xs to sample during evaluation\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    output_dir = \"samples\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='datasets/mnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=1)\n",
    "mnist_testdataset = torchvision.datasets.MNIST(root='datasets/mnist', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_testdataset, batch_size=config.train_batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(t):\n",
    "    return 1 - 0.9999 * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_alpha(t, start=0.2, end=1, tau=1, clip_min=1e-9):\n",
    "    # A gamma function based on cosine function.\n",
    "    v_start = math.cos(start * math.pi / 2) ** (2 * tau)\n",
    "    v_end = math.cos(end * math.pi / 2) ** (2 * tau)\n",
    "    output = torch.cos((t * (end - start) + start) * torch.pi / 2) ** (2 * tau)\n",
    "    output = (v_end - output) / (v_end - v_start)\n",
    "    return torch.clip(output, clip_min, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(clean_x, noise, t):\n",
    "    # it takes the clean xs, the noise and the timesteps as input and returns the noisy xs\n",
    "    alpha = get_alpha(t).to(clean_x.device)\n",
    "    for _ in range(len(clean_x.shape) - 1):\n",
    "        alpha = alpha.unsqueeze(-1)\n",
    "\n",
    "    noisy_x = clean_x * torch.sqrt(alpha)  +  noise * torch.sqrt(1 - alpha)\n",
    "    return noisy_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_show(batch, name, nrow=1):\n",
    "    x_grid = torchvision.utils.make_grid(batch, nrow)\n",
    "    torchvision.utils.save_image(x_grid, name)\n",
    "    display(Image.open(name))\n",
    "\n",
    "sample_batch, sample_y = next(iter(train_dataloader))\n",
    "noise = torch.randn_like(sample_batch)\n",
    "noise_levels = []\n",
    "for i in range(11):\n",
    "    current_batch = forward_diffusion(sample_batch, noise, torch.tensor(i / 10))\n",
    "    noise_levels.append(current_batch)\n",
    "\n",
    "save_and_show(torch.cat(noise_levels), f'forward_diffusion.png', nrow=noise_levels[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.x_size,  # the target x resolution\n",
    "    in_channels=1,  # the number of input channels, 3 for RGB xs\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 128),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"UpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "def train_loop(config, model, forward_diffusion, optimizer, train_dataloader, device):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    sample_batch = next(iter(train_dataloader))[0].to(device)\n",
    "    test_noise = torch.randn_like(sample_batch)\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if epoch * len(train_dataloader) + step in [0, 100, 200, 1000, 5000, 10000, 20000]:\n",
    "                noisy_xs_list = []\n",
    "                sample_batch_reconstructed = []\n",
    "                for noise_level in range(11):\n",
    "                    t = torch.tensor(noise_level / 10)\n",
    "                    alpha = get_alpha(t)\n",
    "                    noisy_xs = forward_diffusion(sample_batch, test_noise, t)\n",
    "                    noisy_xs_list.append(noisy_xs)\n",
    "                    noise_pred = model(noisy_xs, t.to(device), return_dict=False)[0].detach()\n",
    "                    sample_batch_reconstructed.append((noisy_xs - torch.sqrt(1 - alpha) * noise_pred) / torch.sqrt(alpha))\n",
    "                    \n",
    "                save_and_show(torch.cat(sample_batch_reconstructed, 0), f'reconstruction_{epoch}_{step}.png', nrow=sample_batch.shape[0])\n",
    "\n",
    "            clean_xs = batch[0].to(device)\n",
    "            # Sample noise to add to the xs\n",
    "            noise = torch.randn(clean_xs.shape).to(device)\n",
    "            bs = clean_xs.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each x\n",
    "            t = torch.abs(1.0 - torch.rand(bs)).to(device)\n",
    "\n",
    "            # Add noise to the clean xs according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_xs = forward_diffusion(clean_xs, noise, t)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_xs, t, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(config, model, forward_diffusion, optimizer, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'unet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('peal_runs/unet.pt', map_location=torch.device('cpu')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_x(model, current_x, current_timestep, next_timestep, classifier_criterion=None, gradient_scale = 2.0):\n",
    "    current_noise_pred = model(current_x, current_timestep, return_dict=False)[0].detach()\n",
    "\n",
    "    #\n",
    "    alpha_next = get_alpha(next_timestep).to(current_x.device)\n",
    "    alpha_current = get_alpha(current_timestep).to(current_x.device)\n",
    "    for _ in range(len(current_x.shape) - 1):\n",
    "        alpha_next = alpha_next.unsqueeze(-1)\n",
    "        alpha_current = alpha_current.unsqueeze(-1)\n",
    "\n",
    "    #\n",
    "    if not classifier_criterion is None:\n",
    "        x_copy = torch.nn.Parameter(current_x)\n",
    "        loss = classifier_criterion(x_copy)\n",
    "        loss.backward()\n",
    "        current_noise_pred -= gradient_scale * torch.sqrt(1 - alpha_current) * x_copy.grad.detach()\n",
    "\n",
    "    #\n",
    "    next_image = torch.sqrt(1 - alpha_next) * current_noise_pred\n",
    "    next_image += torch.sqrt(alpha_next) * (current_x - torch.sqrt(1 - alpha_current) * current_noise_pred) / torch.sqrt(alpha_current)\n",
    "        \n",
    "    return next_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_ddim(model, noise, num_timesteps, classifier_criterion=None, gradient_scale = 2.0):\n",
    "    # it should take noise, the model and the number of timesteps as input and return the generated images\n",
    "    # Generate the initial image from the noise\n",
    "    noisy_images_list = []\n",
    "    current_x = torch.clone(noise)\n",
    "    \n",
    "    # Perform reverse diffusion for the specified number of timesteps\n",
    "    progress_bar = tqdm(range(num_timesteps - 1))\n",
    "    for t in range(1, num_timesteps + 1)[::-1]:\n",
    "        # Generate the noise for the current timestep\n",
    "        progress_bar.set_description(f\"T {t}\")\n",
    "        timesteps = torch.ones([current_x.shape[0]], dtype=torch.float32).to(noise) * t\n",
    "        current_timestep = timesteps / num_timesteps\n",
    "        next_timestep = (timesteps - 1) / num_timesteps\n",
    "        current_x = get_next_x(model, current_x, current_timestep, next_timestep, classifier_criterion=classifier_criterion, gradient_scale=gradient_scale)\n",
    "        if t % (num_timesteps / 10) == 0:\n",
    "            noisy_images_list.append(current_x)\n",
    "    \n",
    "    noisy_images_list.append(current_x)\n",
    "    save_and_show(torch.cat(noisy_images_list, 0), f'z_to_x.png', nrow=current_x.shape[0])\n",
    "    # Return the generated images\n",
    "    return current_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = reverse_diffusion_ddim(model, torch.randn_like(sample_batch).to(device), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_ddim(model, x, num_timesteps = 100):\n",
    "    noisy_xs_list = []\n",
    "    current_z = torch.clone(x)\n",
    "    \n",
    "    # Perform reverse diffusion for the specified number of timesteps\n",
    "    progress_bar = tqdm(range(num_timesteps - 1))\n",
    "    for t in range(num_timesteps):\n",
    "        # Generate the noise for the current timestep\n",
    "        progress_bar.set_description(f\"T {t} / {num_timesteps}\")\n",
    "        timesteps = torch.ones([current_z.shape[0]], dtype=torch.int32).to(current_z) * t\n",
    "        current_timestep = timesteps / num_timesteps\n",
    "        next_timestep = (timesteps + 1) / num_timesteps\n",
    "        current_z = get_next_x(model, current_z, current_timestep, next_timestep)\n",
    "        if t % (num_timesteps / 10) == 0:\n",
    "            noisy_xs_list.append(current_z)\n",
    "    \n",
    "    noisy_xs_list.append(current_z)\n",
    "    save_and_show(torch.cat(noisy_xs_list, 0), f'x_to_z.png', nrow=current_z.shape[0])\n",
    "    # Return the generated z\n",
    "    return current_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check inversion capabilities\n",
    "reconstruction = reverse_diffusion_ddim(model, forward_diffusion_ddim(model, sample_batch.to(device), 1000), 1000)\n",
    "print(torch.mean(torch.abs(reconstruction.cpu() - sample_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(classifier, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(range(len(train_loader)))\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        progress_bar.set_description(f\"Epoch: {epoch}, Batch: {batch_idx} / {len(train_loader)}, Loss: {loss.item()}\")\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(classifier, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(range(len(test_loader)))\n",
    "    accuracy = torch.tensor(0.0).to(device)\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        prediction = classifier(data).argmax(-1)\n",
    "        accuracy += torch.sum((prediction == target).float())\n",
    "        progress_bar.set_description(f\"Epoch: {epoch}, Batch: {batch_idx} / {len(test_loader)}\")\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Accuracy: {accuracy / len(test_loader.dataset)}\")\n",
    "\n",
    "\n",
    "classifier = Net().to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(12):\n",
    "    train(classifier, device, train_dataloader, optimizer, epoch, criterion)\n",
    "    test(classifier, device, test_dataloader, epoch)\n",
    "\n",
    "torch.save(classifier.state_dict(), \"peal_runs/mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier_criterion(classifier, target, criterion):\n",
    "    def classifier_criterion(x):\n",
    "        return criterion(classifier(x), target)\n",
    "    \n",
    "    return classifier_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_criterion = make_classifier_criterion(classifier, torch.arange(sample_batch.shape[0], dtype=torch.int64).to(device), criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_samples = reverse_diffusion_ddim(model, torch.randn_like(sample_batch).to(device), 3000, classifier_criterion, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_criterion(conditioned_samples.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_criterion2 = make_classifier_criterion(classifier, sample_y.to(device), criterion)\n",
    "classifier_criterion2(conditioned_samples.to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_peal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
