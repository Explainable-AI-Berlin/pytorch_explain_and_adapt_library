{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T08:51:17.806074700Z",
     "start_time": "2023-08-31T08:51:14.621838900Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "# try to import peal and if not installed, add the parent directory to the path\n",
    "try:\n",
    "    import peal\n",
    "\n",
    "except ImportError:\n",
    "    # if peal not installed, but project downloaded locally\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# import basic libraries needed for sure and set the device depending on whether cuda is available or not\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T09:03:22.119733900Z",
     "start_time": "2023-08-31T09:03:22.117728Z"
    }
   },
   "outputs": [],
   "source": [
    "from peal.global_utils import load_yaml_config\n",
    "from peal.data.datasets import SymbolicDataset\n",
    "\n",
    "unpoisened_dataset_config = load_yaml_config('<PEAL_BASE>/configs/data/circle_dataset_diffusion_unpoisened.yaml')\n",
    "\n",
    "dataset = SymbolicDataset(data_dir=unpoisened_dataset_config.dataset_path, mode='train', config=unpoisened_dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.95 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size \n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T08:57:30.487697900Z",
     "start_time": "2023-08-31T08:57:30.477124900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=500):\n",
    "\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        max_len += 1\n",
    "        self.P = torch.zeros(max_len, embed_dim)\n",
    "        freqs = torch.arange(max_len)[:, None] / (torch.pow(10000, torch.arange(0, embed_dim, 2, dtype=torch.float32)/embed_dim))\n",
    "\n",
    "        self.P[:,0::2] = torch.sin(freqs)\n",
    "        self.P[:,1::2] = torch.cos(freqs)\n",
    "        \n",
    "        self.P = self.P[1:]\n",
    "        \n",
    "    def forward(self, t):\n",
    "        return self.P[t]\n",
    "    \n",
    "class ScoreNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(ScoreNetwork, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.layer1 = nn.LazyLinear(embed_dim)\n",
    "        self.layer2 = nn.LazyLinear(embed_dim)\n",
    "        self.layer3 = nn.LazyLinear(embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.layer4 = nn.LazyLinear(input_dim)\n",
    "    \n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.layer1(x) + time_embed\n",
    "        x = F.silu(self.layer2(x))\n",
    "        x = F.silu(self.layer3(x))  \n",
    "        return self.layer4((self.norm(x)))\n",
    "                           \n",
    "class BasicDiscreteTimeModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, num_timesteps: int):\n",
    "        super(BasicDiscreteTimeModel, self).__init__()\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(embed_dim=embed_dim, max_len=num_timesteps)\n",
    "        self.score_network = ScoreNetwork(input_dim=input_dim, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        time_embed = self.positional_embeddings(t)\n",
    "        return self.score_network(x, time_embed)\n",
    "\n",
    "    \n",
    "\n",
    "class CircleDiffusionAdaptor(nn.Module):\n",
    "    def __init__(self, config, dataset, model_dir=None):\n",
    "        super(CircleDiffusionAdaptor, self).__init__()\n",
    "        # self.config = load_yaml_config(config)\n",
    "        self.config = config\n",
    "        \n",
    "        if not model_dir is None:\n",
    "            self.model_dir = model_dir\n",
    "        else:\n",
    "            self.model_dir = config['base_path']\n",
    "        \n",
    "        #if not os.path.exists(model_dir):\n",
    "        #    os.mkdir(model_dir)\n",
    "        #self.model_dir = model_dir\n",
    "        self.input_dim = config['input_dim']\n",
    "        try: \n",
    "            self.num_timesteps = config['num_timesteps']\n",
    "        except KeyError: \n",
    "            pass\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.input_idx = [idx for idx, element in enumerate(self.dataset.attributes) if element not in ['Confounder', 'Target']]\n",
    "        self.target_idx = [idx for idx, element in enumerate(self.dataset.attributes) if element == 'Target']\n",
    "        #data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "        #for idx, key in enumerate(dataset.data):\n",
    "        #    data[idx] = dataset.data[key]\n",
    "        #self.model = self.train_and_load_diffusion(model_name='diffusion.pth')\n",
    "        \n",
    "        def schedules(num_timesteps: int, type: str='linear'):\n",
    " \n",
    "            if type=='linear':\n",
    "                scale = 1000 / num_timesteps\n",
    "                min_var = scale * 1e-4\n",
    "                max_var = scale * 1e-2\n",
    "                return torch.linspace(min_var, max_var, num_timesteps, dtype=torch.float32)\n",
    "            elif type=='cosine':\n",
    "                steps = num_timesteps + 1\n",
    "                x = torch.linspace(0, num_timesteps, steps, dtype=torch.float64)\n",
    "                alphas_cumprod = torch.cos(((x / num_timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "                alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "                betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "                return torch.clip(betas, 0, 0.999)\n",
    "        \n",
    "        betas = schedules(num_timesteps=config['num_timesteps'], type=config['var_schedule'])\n",
    "\n",
    "        self.register_buffer(\"beta\", betas)\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(0))\n",
    "        \n",
    "        \n",
    "    def forward_diffusion(self, clean_x: torch.Tensor, noise: torch.tensor, timestep: torch.Tensor):\n",
    "        \n",
    "        if isinstance(timestep, int):\n",
    "            timestep = torch.tensor([timestep])\n",
    "            alpha_bar_t = self.alpha_bar[timestep].repeat(clean_x.shape[0])[:, None]\n",
    "        else:\n",
    "            alpha_bar_t = self.alpha_bar[timestep][:, None]\n",
    "        mu = torch.sqrt(alpha_bar_t)\n",
    "        std = torch.sqrt(1 - alpha_bar_t)\n",
    "        noisy_x = mu * clean_x + std * noise\n",
    "        return noisy_x\n",
    "    \n",
    "\n",
    "    def reverse_diffusion_ddpm(self, noisy_x: torch.Tensor, model: nn.Module, timestep: torch.Tensor):\n",
    "        alpha_t = self.alpha[timestep].repeat(noisy_x.shape[0])[:, None]\n",
    "        alpha_bar_t = self.alpha_bar[timestep].repeat(noisy_x.shape[0])[:, None]\n",
    "        beta_t = 1 - alpha_t\n",
    "        eps_hat = model(x=noisy_x, t=timestep)\n",
    "        posterior_mean = (1 / torch.sqrt(alpha_t)) * (noisy_x - (beta_t / torch.sqrt(1 - alpha_bar_t) * eps_hat))\n",
    "        z = torch.randn_like(noisy_x)\n",
    "        \n",
    "        if timestep > 0:\n",
    "            alpha_bar_t_minus_1 = self.alpha_bar[timestep-1].repeat(noisy_x.shape[0])[:, None]\n",
    "            sigma_t = beta_t * (1 - alpha_bar_t_minus_1) / (1 - alpha_bar_t)\n",
    "            denoised_x = posterior_mean + torch.sqrt(sigma_t)*z #* z * (timestep > 0))  # variance = beta_t\n",
    "        else:\n",
    "            denoised_x = posterior_mean\n",
    "                                           \n",
    "        return denoised_x\n",
    "    \n",
    "    def train_and_load_diffusion(self, model_name='diffusion.pt', mode=None):\n",
    "        \n",
    "        self.model_path = os.path.join(self.model_dir, model_name)\n",
    "        model = BasicDiscreteTimeModel(input_dim=self.config['input_dim'], embed_dim=self.config['embed_dim'], num_timesteps=self.config['num_timesteps'])\n",
    "        if model_name in os.listdir(self.model_dir) and not mode == \"train\":\n",
    "            model.load_state_dict(torch.load(self.model_path))\n",
    "            logging.info(f'Model found with path {self.model_path}')\n",
    "        elif model_name not in os.listdir(self.model_dir) and mode != 'train':\n",
    "            logging.info('Model not found. Please run train_and_load_diffusion method and set its argument mode=\"train\" ')\n",
    "        else:\n",
    "            logging.info(\n",
    "                f'Training model with path {self.model_path}'\n",
    "            )\n",
    "        \n",
    "        def diffusion_loss(model: nn.Module, clean_x: torch.Tensor) -> torch.Tensor:\n",
    "            t = torch.randint(self.num_timesteps, (clean_x.shape[0],))\n",
    "            eps_t = torch.randn_like(clean_x)\n",
    "            alpha_bar_t = self.alpha_bar[t][:, None]\n",
    "            x_t = self.forward_diffusion(clean_x=clean_x, noise=eps_t, timestep=t)\n",
    "            eps_hat = model(x=x_t, t=t)\n",
    "            loss_diff = nn.MSELoss(reduction='sum')(eps_hat, eps_t)\n",
    "            \n",
    "            return loss_diff\n",
    "                    \n",
    "        def run_epoch(model: nn.Module, dataloader: torch.utils.data.dataloader.DataLoader):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for x, _ in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = diffusion_loss(model, x[:, self.input_idx])\n",
    "                epoch_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            return epoch_loss / len(dataloader.dataset)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            model.train()\n",
    "            num_epochs = self.config['num_epochs']\n",
    "            dataloader = DataLoader(self.dataset, batch_size=self.config['batch_size'], shuffle=True)\n",
    "            learning_rate = 1e-4\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            losses = []\n",
    "            for i in trange(num_epochs):\n",
    "                epoch_loss = 0.0\n",
    "                for x, _ in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = diffusion_loss(model, x[:, self.input_idx])\n",
    "                    epoch_loss += loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_loss = epoch_loss / len(dataloader.dataset)\n",
    "                print(f'Epoch: {i}, train_loss: {train_loss}')\n",
    "                losses.append(train_loss.detach().numpy())\n",
    "            \n",
    "            torch.save(model.state_dict(), self.model_path) \n",
    "            \n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_ddpm(self, model: nn.Module, n_samples: int = 256, label=None):\n",
    "        \"\"\"\n",
    "        iteratively denoises pure noise to produce a list of denoised samples at each timestep\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        x_pred = []\n",
    "        x = torch.randn(n_samples, self.input_dim)\n",
    "        x_pred.append(x)\n",
    "        \n",
    "        step = 1\n",
    "        for t in reversed(range(0, self.num_timesteps, step)):\n",
    "                \n",
    "            x = self.reverse_diffusion_ddpm(noisy_x=x, model=model, timestep=t)\n",
    "                \n",
    "            x_pred.append(x)\n",
    "        return x_pred\n",
    "    \n",
    "    def sample_x(self, batch_size=1):\n",
    "        x = self.sample_ddpm(model=self.model, n_samples=batch_size)[-1] \n",
    "        return x\n",
    "\n",
    "    def sample_counterfactual_ddpm(self, clean_batch: torch.Tensor, model: nn.Module, classifier: nn.Module, num_noise_steps: int, target_classes: int, classifier_grad_weight: float):\n",
    "        \n",
    "        \n",
    "        classifier.eval()\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        # DEFINE BATCH SIZE AND COUNTERFACTUAL CLASS\n",
    "        bs = clean_batch.shape[0]\n",
    "\n",
    "        # COMPUTE CLEAN GRADIENTS FOR THE FIRST STEP\n",
    "        \n",
    "        classifier_criterion = lambda x: F.cross_entropy(classifier(x), target_classes)\n",
    "        clean_batch_copy = torch.nn.Parameter(clean_batch)\n",
    "        loss = classifier_criterion(clean_batch_copy)\n",
    "        loss.backward()\n",
    "        clean_grad = classifier_grad_weight * clean_batch_copy.grad.detach()\n",
    "        \n",
    "        # PERFORMING FORWARD DIFFUSION UNTIL NUM_NOISE_STEPS\n",
    "        eps_t = torch.randn_like(clean_batch)\n",
    "        next_z = self.forward_diffusion(clean_x=clean_batch, noise=eps_t, timestep=num_noise_steps)\n",
    "        counterfactuals = [] # total counterfactuals\n",
    "        counterfactuals.append(clean_batch)\n",
    "        guided_grads = []  # guided grads at the first step\n",
    "        unconditional_grads = [] # diffusion grads at the first step\n",
    "        total_series = [] # contains evolution from noisy to cleaned instance for each data point\n",
    "        for i in tqdm(range(0, num_noise_steps)[::-1]):\n",
    "            # Denoise z_t to create z_t-1 (next z)\n",
    "            alpha_i = self.alpha[i].repeat(bs)[:, None]\n",
    "            alpha_bar_i = self.alpha_bar[i].repeat(bs)[:, None]\n",
    "            sigma_i = torch.sqrt(1 - self.alpha[i])\n",
    "            eps_hat = model(next_z, i)\n",
    "\n",
    "            # Unconditional mean\n",
    "            unconditional_grad = -eps_hat / torch.sqrt(1 - alpha_bar_i)\n",
    "            z_t_mean = (next_z + unconditional_grad * (1 - alpha_i)) / torch.sqrt(alpha_i)\n",
    "\n",
    "            # Guided mean\n",
    "            z_t_mean -= sigma_i * (clean_grad / torch.sqrt(alpha_bar_i))\n",
    "\n",
    "            if i > 0:\n",
    "                next_z = z_t_mean + (sigma_i * torch.randn_like(clean_batch))\n",
    "            else:\n",
    "                next_z = z_t_mean\n",
    "\n",
    "            next_x = next_z.clone()\n",
    "            # Denoise to create a cleaned x (next x)\n",
    "            series = []\n",
    "            series.append(next_x.detach())\n",
    "            for t in range(0, i)[::-1]:\n",
    "                if i == 0:\n",
    "                    break\n",
    "                next_x = self.reverse_diffusion_ddpm(noisy_x=next_x, model=model, timestep=t)\n",
    "                series.append(next_x.detach())\n",
    "            total_series.append(series)\n",
    "            guided_grads.append(-sigma_i * clean_grad.detach() / torch.sqrt(alpha_bar_i))\n",
    "            unconditional_grads.append(unconditional_grad.detach() * (1 - alpha_i) / torch.sqrt(alpha_i) )\n",
    "            \n",
    "            \n",
    "            if i != 0:\n",
    "                counterfactuals.append(next_x.detach())\n",
    "\n",
    "            # Gradient wrt denoised image (next_x)\n",
    "            next_x_copy = torch.nn.Parameter(next_x.clone())\n",
    "            loss = classifier_criterion(next_x_copy)\n",
    "            loss.backward()\n",
    "            clean_classifier_grad = next_x_copy.grad.detach()\n",
    "            clean_grad = classifier_grad_weight * clean_classifier_grad\n",
    "            del clean_classifier_grad\n",
    "            next_x_copy.grad.zero_()\n",
    "            \n",
    "            \n",
    "        counterfactuals = torch.stack(counterfactuals).permute(1, 0, 2) \n",
    "        guided_grads = torch.stack(guided_grads).permute(1, 0, 2) \n",
    "        unguided_grads = torch.stack(unconditional_grads).permute(1, 0, 2)\n",
    "        \n",
    "        self.counterfactuals_series = counterfactuals\n",
    "        self.guided_grads = guided_grads\n",
    "        self.unguided_grads = unguided_grads\n",
    "        \n",
    "        return counterfactuals, guided_grads, unguided_grads, total_series\n",
    "\n",
    "    \n",
    "    def discard_counterfactuals(self, counterfactuals, classifier, target_classes, target_confidence, minimal_counterfactuals, tolerance=0.1):\n",
    "        \n",
    "        # compute distance of current minimal_counterefactuals from radius 1.0\n",
    "        #current_counterfactual_distance_from_manifold = torch.abs((torch.pow(minimal_counterfactuals, 2).sum(dim=-1) - 1.0))\n",
    "        \n",
    "        for i in range(len(counterfactuals)):  \n",
    "\n",
    "            # compute classifier  for all the counterfactuals for each point\n",
    "            new_counterfactuals_confidence = classifier(counterfactuals[i]).softmax(dim=-1)[:, target_classes[i]]\n",
    "            \n",
    "            \n",
    "            # check if new counterfactuals satisfy the confidence constraint\n",
    "            new_confidence_satisfied = new_counterfactuals_confidence > target_confidence\n",
    "            \n",
    "            new_confidence_satisfied_indices = torch.nonzero(new_counterfactuals_confidence > target_confidence)\n",
    "            \n",
    "            current_confidence_satisfied = classifier(minimal_counterfactuals[i:i+1]).softmax(dim=-1)[0][target_classes[i]].item() > target_confidence\n",
    "            \n",
    "            # if current counterfactual satisfies confidence and tolerance, maintain status quo \n",
    "          \n",
    "            if new_confidence_satisfied_indices.nelement() != 0:\n",
    "                print('new confidence satisfied')\n",
    "                print(new_confidence_satisfied_indices[0].item())\n",
    "                minimal_counterfactuals[i] = counterfactuals[i][new_confidence_satisfied_indices[0].item()]\n",
    "                \n",
    "            else:\n",
    "                print('neither current nor new confidence satisfied')\n",
    "                minimal_counterfactuals[i] = counterfactuals[i][-1]\n",
    "\n",
    "        return minimal_counterfactuals\n",
    "        \n",
    "        \n",
    "    def edit(\n",
    "        self,\n",
    "        x_in: torch.Tensor,\n",
    "        target_confidence_goal: float,\n",
    "        target_classes: torch.Tensor,\n",
    "        classifier: nn.Module\n",
    "    ) -> Tuple[list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]]:\n",
    "        \n",
    "        self.original_sample = x_in\n",
    "        #minimal_counterfactuals = torch.zeros(size=x_in.shape)\n",
    "\n",
    "        scales = self.config['grad_scales']\n",
    "        noise_steps = self.config['noise_steps_for_counterfactuals']\n",
    "        \n",
    "        minimal_counterfactuals = x_in.clone()\n",
    "        \n",
    "        for it in range(self.config['num_iterations']):\n",
    "            for steps in noise_steps:\n",
    "\n",
    "                for s in scales:\n",
    "\n",
    "                    counterfactuals, guided_grads, unguided_grads, total_series = self.sample_counterfactual_ddpm(clean_batch=minimal_counterfactuals, model=self.model, classifier=classifier, num_noise_steps=steps, target_classes=target_classes, classifier_grad_weight=s)\n",
    "\n",
    "                    minimal_counterfactuals = self.discard_counterfactuals(counterfactuals=counterfactuals, classifier=classifier, target_confidence=target_confidence_goal, target_classes=target_classes, minimal_counterfactuals=minimal_counterfactuals)\n",
    "                    self.counterfactuals = minimal_counterfactuals\n",
    "                    flip_rate = sum(classifier(minimal_counterfactuals).softmax(dim=-1).argmax(dim=-1) != classifier(x_in).softmax(dim=-1).argmax(dim=-1)) / len(x_in)\n",
    "                    self.plot_counterfactuals()\n",
    "                    plt.title(f'Noise Steps: {steps}, Gradient Scale: {s}, Flip Rate: {round(flip_rate.item(),3)}')\n",
    "                    #plt.show()\n",
    "        list_counterfactuals = [row_tensor for row_tensor in minimal_counterfactuals]\n",
    "        diff_latent = x_in - minimal_counterfactuals\n",
    "        \n",
    "        confidences = classifier(minimal_counterfactuals).softmax(dim=-1)\n",
    "        y_target_end_confidence = [confidences[i][target_classes[i]].detach() for i in range(len(minimal_counterfactuals))]\n",
    "        x_list = [row_tensor for row_tensor in x_in]\n",
    "        \n",
    "        #self.counterfactuals = minimal_counterfactuals\n",
    "        \n",
    "        return list_counterfactuals, diff_latent, y_target_end_confidence, x_list\n",
    "    \n",
    "    \n",
    "    def plot_counterfactuals(self):\n",
    "        #plt.figure(figsize=(5,5))\n",
    "        data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "        for idx, key in enumerate(dataset.data):\n",
    "            data[idx] = dataset.data[key]\n",
    "        self.data = data\n",
    "        plt.scatter(data[:,self.input_idx[0]], data[:,self.input_idx[1]], c=np.where(data[:,self.target_idx] == 0, 'lightcyan', 'lightgray')[0])\n",
    "        for i, point in enumerate(self.counterfactuals):\n",
    "            plt.scatter(self.original_sample[i, 0], self.original_sample[i, 1], color='green', label='start')\n",
    "            plt.scatter(point[0], point[1], color='red', label='end')\n",
    "            plt.arrow(\n",
    "                self.original_sample[i,0], self.original_sample[i, 1], # plot the original point plus arrow until (j+granularity)th point\n",
    "                point[0] - self.original_sample[i, 0], \n",
    "                point[1] - self.original_sample[i, 1],\n",
    "                head_width=0.05, head_length=0.05, fc='blue', ec='blue',\n",
    "\n",
    "            )   \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model found with path peal_runs/artificial_circle_diffusion_generator/diffusion.pt\n"
     ]
    }
   ],
   "source": [
    "from peal.configs.adaptors.adaptor_template import AdaptorConfig\n",
    "adaptor_config = load_yaml_config('<PEAL_BASE>/configs/adaptors/circle_diffusion.yaml', AdaptorConfig)\n",
    "adaptor = CircleDiffusionAdaptor(config=load_yaml_config(adaptor_config).generator, dataset=train_set.dataset, model_dir=None)\n",
    "adaptor.train_and_load_diffusion(model_name='diffusion.pt')#, mode='train')\n",
    "student = torch.load('peal_runs/artificial_circle_poisened_classifier/model.cpl')\n",
    "teacher = torch.load('peal_runs/artificial_circle_unpoisened_classifier/model.cpl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = adaptor.sample_ddpm(adaptor.model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4)) \n",
    "\n",
    "indices = np.array([0, 400, 450, 470, 500], dtype=np.int)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axs[i].scatter(series[idx][:,0], series[idx][:,1], s=14.0, color='teal')\n",
    "    if idx == 500:\n",
    "        axs[i].set_title(f'Final')\n",
    "    else:\n",
    "        axs[i].set_title(f'Timestep {500-idx}')\n",
    "plt.suptitle('Diffusion Samples', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "samples = adaptor.sample_x(batch_size=500)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(samples[:,0], samples[:,1], s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_distance(samples):\n",
    "    radius = 1\n",
    "    return (((samples.pow(2)).sum(dim=-1) - radius).pow(2)).mean()\n",
    "\n",
    "def angle_cdf(samples):\n",
    "    scores = abs(samples[:, 1] / samples[:, 0])\n",
    "\n",
    "    first_quad_mask = (samples[:, 0] > 0) & (samples[:, 1] > 0)\n",
    "    second_quad_mask = (samples[:, 0] < 0) & (samples[:, 1] > 0)\n",
    "    third_quad_mask = (samples[:, 0] < 0) & (samples[:, 1] < 0)\n",
    "    fourth_quad_mask = (samples[:, 0] > 0) & (samples[:, 1] < 0)\n",
    "    theta_1 = torch.atan(scores) * first_quad_mask\n",
    "    theta_1 = theta_1[theta_1 != 0]\n",
    "    theta_2 = (torch.pi - torch.atan(scores)) * second_quad_mask\n",
    "    theta_2 = theta_2[theta_2 != 0]\n",
    "    theta_3 = (torch.pi + torch.atan(scores)) * third_quad_mask\n",
    "    theta_3 = theta_3[theta_3 != 0]\n",
    "    theta_4 = (2 * torch.pi - torch.atan(scores)) * fourth_quad_mask\n",
    "    theta_4 = theta_4[theta_4 != 0]\n",
    "    thetas, indices = torch.cat([theta_1, theta_2, theta_3, theta_4]).sort(dim=-1)\n",
    "\n",
    "    return thetas\n",
    "\n",
    "def circle_ks(samples):\n",
    "    dist = torch.distributions.uniform.Uniform(0, 2*torch.pi)\n",
    "    sample_thetas = angle_cdf(samples)\n",
    "    \n",
    "    ecdf = torch.arange(len(samples)) / len(samples)\n",
    "    true_cdf = dist.cdf(sample_thetas)\n",
    "    return torch.max(torch.abs(dist.cdf(sample_thetas) - ecdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.linspace(5, 30000, 50, dtype=np.int)\n",
    "distances = []\n",
    "ks_stats = []\n",
    "for size in sizes:\n",
    "    dist = 0.0\n",
    "    ks = 0.0\n",
    "    for it in range(5):\n",
    "        samples = adaptor.sample_x(batch_size=size)\n",
    "        dist += circle_distance(samples)\n",
    "        ks += circle_ks(samples)\n",
    "    distances.append(dist/5)\n",
    "    ks_stats.append(ks/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from peal.generators.interfaces import EditCapableGenerator\n",
    "%matplotlib inline\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim: int, encoder_dims: list, decoder_dims: list, latent_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i, dim in enumerate(encoder_dims):\n",
    "            self.encoder.add_module(f'layer_{i + 1}', nn.Sequential(nn.LazyLinear(dim), nn.SiLU()))\n",
    "        self.encoder.add_module('norm_encoder', nn.LayerNorm(dim))\n",
    "\n",
    "        self.latent_mean = nn.Sequential(nn.LazyLinear(latent_dim))\n",
    "        self.latent_logvar = nn.Sequential(nn.LazyLinear(latent_dim))\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i, dim in enumerate(decoder_dims):\n",
    "            self.decoder.add_module(f'layer_{i + 1}', nn.Sequential(nn.LazyLinear(dim), nn.SiLU()))\n",
    "        self.decoder.add_module('norm_decoder', nn.LayerNorm(dim))\n",
    "        self.decoder.add_module(f'to_original', nn.Sequential(nn.LazyLinear(input_dim)))\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            z = mean + torch.exp(logvar * 0.5) * torch.randn_like(logvar)\n",
    "        else:\n",
    "            z = mean\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.latent_mean(x), self.latent_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        eps = torch.randn([num_samples, self.latent_dim])\n",
    "        return self.decode(eps).detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "\n",
    "class CircleVAEAdaptor(EditCapableGenerator):\n",
    "    def __init__(self, config, dataset, model_dir=None):\n",
    "        super(CircleVAEAdaptor, self).__init__()\n",
    "        self.config = config\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if not model_dir is None:\n",
    "            self.model_dir = model_dir\n",
    "        else:\n",
    "            self.model_dir = config['base_path']\n",
    "\n",
    "        #self.input_dim = config.input_dim\n",
    "        #self.encoder_dims = config.encoder_dims\n",
    "        #self.decoder_dims = config.decoder_dims\n",
    "        #self.latent_dim = config.latent_dim\n",
    "        \n",
    "        self.input_dim = config['input_dim']\n",
    "        self.encoder_dims = config['encoder_dims']\n",
    "        self.decoder_dims = config['decoder_dims']\n",
    "        self.latent_dim = config['latent_dim']\n",
    "\n",
    "        self.train_and_load_vae(model_name=config['model_name'])\n",
    "\n",
    "        self.input_idx = [\n",
    "            idx\n",
    "            for idx, element in enumerate(self.dataset.attributes)\n",
    "            if element not in [\"Confounder\", \"Target\"]\n",
    "        ]\n",
    "        self.target_idx = [\n",
    "            idx\n",
    "            for idx, element in enumerate(self.dataset.attributes)\n",
    "            if element == \"Target\"\n",
    "        ]\n",
    "\n",
    "    def train_and_load_vae(self, model_name='vae.pt', mode=None):\n",
    "        self.model_path = os.path.join(self.model_dir, model_name)\n",
    "        model = VAE(input_dim=self.input_dim, encoder_dims=self.encoder_dims, decoder_dims=self.decoder_dims,\n",
    "                    latent_dim=self.latent_dim)\n",
    "\n",
    "        if model_name in os.listdir(self.model_dir) and not mode == \"train\":\n",
    "            model.load_state_dict(torch.load(self.model_path))\n",
    "            logging.info(f'Model found with path {self.model_path}')\n",
    "        elif model_name not in os.listdir(self.model_dir) and mode != 'train':\n",
    "            logging.info('Model not found. Please run train_and_load_vae method and set its argument mode=\"train\" ')\n",
    "        else:\n",
    "            logging.info(\n",
    "                f'Training model with path {self.model_path}'\n",
    "            )\n",
    "\n",
    "        def VAELoss(x, x_hat, mean, logvar, beta=self.config['beta']):\n",
    "            kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim=1), dim=0)\n",
    "            reconstruction_loss = F.mse_loss(x, x_hat)\n",
    "            return reconstruction_loss + beta * kl_loss\n",
    "\n",
    "        def train(model, data_loader, epochs):\n",
    "            model.train()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                total_loss = 0.0\n",
    "                for x, y in data_loader:\n",
    "                    x = x\n",
    "                    optimizer.zero_grad()\n",
    "                    x_hat, mean, logvar = model(x[:, self.input_idx])\n",
    "                    loss = VAELoss(x[:, self.input_idx], x_hat=x_hat, mean=mean, logvar=logvar)\n",
    "                    total_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "\n",
    "            return x, x_hat\n",
    "\n",
    "        if mode == 'train':\n",
    "            model.train()\n",
    "            dataloader = DataLoader(self.dataset, batch_size=self.config['batch_size'], shuffle=True)\n",
    "            train(model=model, data_loader=dataloader, epochs=self.config['num_epochs'])\n",
    "            torch.save(model.state_dict(), self.model_path)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def sample_x(self, batch_size=1):\n",
    "        return self.model.sample(num_samples=batch_size).detach()\n",
    "\n",
    "    def DIVE(self, clean_batch, target_classes, model, classifier):\n",
    "\n",
    "        classifier.eval()\n",
    "\n",
    "        lasso_weight = self.config['lasso_weight']\n",
    "        reconstruction_weight = self.config['reconstruction_weight']\n",
    "\n",
    "        batch_size, _ = clean_batch.size()\n",
    "        latent_dim = self.latent_dim\n",
    "\n",
    "        mean, logvar = self.model.encode(clean_batch)\n",
    "        z = model.reparameterize(mean, logvar).detach()  # no grads required for latents\n",
    "\n",
    "        epsilon = torch.randn_like(z, requires_grad=True)\n",
    "        epsilon.data *= 0.01\n",
    "        optimizer = torch.optim.Adam([epsilon], lr=self.config['lr_counterfactual'], weight_decay=0)\n",
    "        \n",
    "        z_perturbed = z + epsilon \n",
    "        list_z = []\n",
    "        list_counterfactuals = []\n",
    "        list_z.append(z[0])\n",
    "        list_counterfactuals.append(clean_batch[0])\n",
    "        for it in range(self.config['num_iterations']):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            decoded = model.decode(z_perturbed)\n",
    "\n",
    "            classifier_criterion = lambda x: F.cross_entropy(classifier(x), target_classes)\n",
    "            loss_attack = classifier_criterion(decoded)\n",
    "            recon_regularizer = reconstruction_weight * torch.abs((clean_batch - decoded).mean(dim=-1)).sum()\n",
    "            lasso_regularizer = lasso_weight * (torch.abs(z_perturbed - z)).sum()\n",
    "            regularizer = recon_regularizer + lasso_regularizer\n",
    "\n",
    "            loss = loss_attack + regularizer\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            z_perturbed = z + epsilon\n",
    "            x = model.decode(z_perturbed).detach()\n",
    "            list_z.append(z_perturbed.detach()[0])\n",
    "            list_counterfactuals.append(x[0])\n",
    "\n",
    "        return clean_batch, x, torch.stack(list_z), torch.stack(list_counterfactuals)\n",
    "    \n",
    "    def discard_counterfactuals(self, counterfactuals, classifier, target_classes, target_confidence, minimal_counterfactuals):\n",
    "        for i in range(len(counterfactuals)):\n",
    "            \n",
    "            new_counterfactuals_confidence = classifier(counterfactuals[i]).softmax(dim=-1)[:, target_classes[i]]\n",
    "            \n",
    "            new_confidence_satisfied = new_counterfactuals_confidence > target_confidence\n",
    "            \n",
    "            new_confidence_satisfied_indices = torch.nonzero(new_counterfactuals_confidence > target_confidence)\n",
    "            \n",
    "            current_confidence_satisfied = classifier(minimal_counterfactuals[i:i+1]).softmax(dim=-1)[0][target_classes[i]].item() > target_confidence\n",
    "             \n",
    "            \n",
    "\n",
    "    def edit(\n",
    "            self,\n",
    "            x_in: torch.Tensor,\n",
    "            target_confidence_goal: float,\n",
    "            target_classes: torch.Tensor,\n",
    "            classifier: torch.nn.Module,\n",
    "            **kwargs,\n",
    "    ) -> Tuple[\n",
    "        list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Edit a batch of samples to achieve a target confidence goal.\n",
    "        Args:\n",
    "            x_in: Batch of samples to edit.\n",
    "            target_confidence_goal: Target confidence goal.\n",
    "            target_classes: Target classes for each sample in the batch.\n",
    "            classifier: Classifier to use for confidence estimation.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        Returns:\n",
    "            Tuple of (edited samples, confidence estimates, number of iterations, number of queries).\n",
    "        \"\"\"\n",
    "\n",
    "        list_counterfactuals = torch.zeros_like(x_in)\n",
    "        y_target_end_confidence = torch.zeros([x_in.shape[0]])\n",
    "        counterfactuals = x_in\n",
    "        for i in range(len(x_in)):\n",
    "            #while True:\n",
    "            _, counterfactual, _ ,_ = self.DIVE(x_in[i:i + 1], target_classes[i:i + 1], self.model, classifier)\n",
    "            current_confidence = classifier(counterfactual).softmax(dim=-1)[0][target_classes[i].item()].item()\n",
    "                #if current_confidence > target_confidence_goal:\n",
    "                #    break\n",
    "            y_target_end_confidence[i] = current_confidence\n",
    "            list_counterfactuals[i] = counterfactual\n",
    "\n",
    "        diff_latent = x_in - list_counterfactuals\n",
    "\n",
    "        x_list = [row_tensor for row_tensor in x_in]\n",
    "\n",
    "        return list(list_counterfactuals), diff_latent, y_target_end_confidence, x_list\n",
    "\n",
    "def circle_distance(samples):\n",
    "    radius = 1\n",
    "    return (((samples.pow(2)).sum(dim=-1) - radius).pow(2)).mean()\n",
    "\n",
    "def angle_cdf(samples):\n",
    "    scores = abs(samples[:, 1] / samples[:, 0])\n",
    "\n",
    "    first_quad_mask = (samples[:, 0] > 0) & (samples[:, 1] > 0)\n",
    "    second_quad_mask = (samples[:, 0] < 0) & (samples[:, 1] > 0)\n",
    "    third_quad_mask = (samples[:, 0] < 0) & (samples[:, 1] < 0)\n",
    "    fourth_quad_mask = (samples[:, 0] > 0) & (samples[:, 1] < 0)\n",
    "    theta_1 = torch.atan(scores) * first_quad_mask\n",
    "    theta_1 = theta_1[theta_1 != 0]\n",
    "    theta_2 = (torch.pi - torch.atan(scores)) * second_quad_mask\n",
    "    theta_2 = theta_2[theta_2 != 0]\n",
    "    theta_3 = (torch.pi + torch.atan(scores)) * third_quad_mask\n",
    "    theta_3 = theta_3[theta_3 != 0]\n",
    "    theta_4 = (2 * torch.pi - torch.atan(scores)) * fourth_quad_mask\n",
    "    theta_4 = theta_4[theta_4 != 0]\n",
    "    thetas, indices = torch.cat([theta_1, theta_2, theta_3, theta_4]).sort(dim=-1)\n",
    "\n",
    "    return thetas\n",
    "\n",
    "def circle_ks(samples):\n",
    "    dist = torch.distributions.uniform.Uniform(0, 2*torch.pi)\n",
    "    sample_thetas = angle_cdf(samples)\n",
    "    \n",
    "    ecdf = torch.arange(len(samples)) / len(samples)\n",
    "    true_cdf = dist.cdf(sample_thetas)\n",
    "    return torch.max(torch.abs(dist.cdf(sample_thetas) - ecdf))\n",
    "\n",
    "adaptor_config = load_yaml_config('<PEAL_BASE>/configs/adaptors/circle_vae.yaml')\n",
    "adaptor = CircleVAEAdaptor(config=adaptor_config.generator, dataset=train_set.dataset, model_dir=None)\n",
    "adaptor.train_and_load_vae(model_name='vae_beta_0.05.pt')#, mode='train')\n",
    "\n",
    "sizes = np.linspace(5, 30000, 50, dtype=np.int)\n",
    "distances_vae = []\n",
    "ks_vae = []\n",
    "for size in sizes:\n",
    "    dist = 0.0\n",
    "    ks = 0.0\n",
    "    for it in range(5):\n",
    "        samples = adaptor.sample_x(batch_size=size)\n",
    "        dist += circle_distance(samples)\n",
    "        ks += circle_ks(samples)\n",
    "    distances_vae.append(dist/5)\n",
    "    ks_vae.append(ks/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 7)) \n",
    "\n",
    "axs[0].plot(np.array(sizes), distances, label='DDPM')\n",
    "axs[0].plot(np.array(sizes), distances_vae, label='VAE')\n",
    "#axs[0].plot(np.array(sizes), distances_bvae, label='VAE_beta')\n",
    "#axs[0].plot(np.array(sizes), distances_bvae5, label='VAE_beta5')\n",
    "axs[0].set_title(f'Closeness to the Manifold', fontsize='15')\n",
    "\n",
    "axs[1].plot(np.array(sizes), np.array(ks_stats), label='DDPM')\n",
    "axs[1].plot(np.array(sizes), np.array(ks_vae), label='VAE')\n",
    "axs[1].plot(np.array(sizes), -1*np.array(ks_bvae), label='VAE_beta')\n",
    "axs[1].plot(np.array(sizes), -1*np.array(ks_bvae5), label='VAE_beta5')\n",
    "axs[1].set_title(f'Diversity', fontsize='15')\n",
    "axs[0].legend(fontsize='12')\n",
    "axs[1].legend(fontsize='12')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peal.adaptors.counterfactual_knowledge_distillation import (\n",
    "    CounterfactualKnowledgeDistillation,\n",
    ")\n",
    "from peal.global_utils import load_yaml_config, add_class_arguments, integrate_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "for idx, key in enumerate(dataset.data):\n",
    "    data[idx] = dataset.data[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model found with path peal_runs/artificial_circle_diffusion_generator/diffusion.pt\n"
     ]
    }
   ],
   "source": [
    "cfkd = CounterfactualKnowledgeDistillation(adaptor_config=adaptor_config, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(cfkd.test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_counterfactuals, latent_diff, confidence, list_x = cfkd.generator.edit(x_in=x[:10], target_confidence_goal=0.7, source_classes=y[:10], target_classes=1-y[:10].type(torch.long), classifier=cfkd.student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], color='lightgray')\n",
    "for i, point in enumerate(list_counterfactuals):\n",
    "    plt.scatter(list_x[i][0], list_x[i][1], color='green')\n",
    "    plt.scatter(list_counterfactuals[i][0], list_counterfactuals[i][1], color='red')\n",
    "    plt.arrow(\n",
    "        list_x[i][0], list_x[i][1], # plot the original point plus arrow until (j+granularity)th point\n",
    "        list_counterfactuals[i][0] - list_x[i][0], \n",
    "        list_counterfactuals[i][1] - list_x[i][1],\n",
    "        head_width=0.05, head_length=0.05, fc='blue', ec='blue',\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptor Config: <peal.configs.adaptors.adaptor_template.AdaptorConfig object at 0x7f671b85bbb0>\n",
      "Create base_dir in: peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100\n",
      "Generator performance: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/26 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "test_correct: 1.0, it: 0:   4%|██▏                                                       | 1/26 [00:00<00:01, 13.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/31 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|████▏                                                                               | 3/60 [00:00<00:02, 22.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████████▊                                                                          | 7/60 [00:00<00:01, 29.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|███████████████▏                                                                   | 11/60 [00:00<00:01, 31.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|████████████████████▊                                                              | 15/60 [00:00<00:01, 33.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███████████████████████████▋                                                       | 20/60 [00:00<00:01, 37.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████████████████████▏                                                 | 24/60 [00:00<00:00, 36.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████▉                                        | 31/60 [00:00<00:00, 45.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 40/60 [00:00<00:00, 57.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:01<00:00, 57.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "  0%|                                                                                            | 0/31 [00:01<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/155 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███████                                                                             | 5/60 [00:00<00:01, 42.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█████████████▊                                                                     | 10/60 [00:00<00:01, 41.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|████████████████████▊                                                              | 15/60 [00:00<00:01, 42.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███████████████████████████▋                                                       | 20/60 [00:00<00:00, 43.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|███████████████████████████████████▉                                               | 26/60 [00:00<00:00, 48.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████████████████████████████████████████████▋                                     | 33/60 [00:00<00:00, 52.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 40/60 [00:00<00:00, 57.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 64.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|████▏                                                                               | 3/60 [00:00<00:01, 29.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|███████████▏                                                                        | 8/60 [00:00<00:01, 37.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|█████████████████▉                                                                 | 13/60 [00:00<00:01, 41.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████████████████▎                                                        | 19/60 [00:00<00:00, 46.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|██████████████████████████████████▌                                                | 25/60 [00:00<00:00, 49.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████████████████████████████▎                                      | 32/60 [00:00<00:00, 56.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|████████████████████████████████████████████████████████▋                          | 41/60 [00:00<00:00, 66.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 69.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                              | 4/60 [00:00<00:01, 39.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|████████████▌                                                                       | 9/60 [00:00<00:01, 41.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|███████████████████▎                                                               | 14/60 [00:00<00:01, 38.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███████████████████████████▋                                                       | 20/60 [00:00<00:00, 43.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████████████████████████▎                                             | 27/60 [00:00<00:00, 50.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|███████████████████████████████████████████████                                    | 34/60 [00:00<00:00, 54.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████████▍                       | 43/60 [00:00<00:00, 63.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 65.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                              | 4/60 [00:00<00:01, 34.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|███████████▏                                                                        | 8/60 [00:00<00:01, 37.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|█████████████████▉                                                                 | 13/60 [00:00<00:01, 41.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|████████████████████████▉                                                          | 18/60 [00:00<00:00, 43.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███████████████████████████████▊                                                   | 23/60 [00:00<00:00, 44.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████████████████████████                                           | 29/60 [00:00<00:00, 49.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 36/60 [00:00<00:00, 55.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 65.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███████                                                                             | 5/60 [00:00<00:01, 42.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█████████████▊                                                                     | 10/60 [00:00<00:01, 39.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|████████████████████▊                                                              | 15/60 [00:00<00:01, 39.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████████████████████                                                      | 21/60 [00:00<00:00, 44.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████████████████████████▎                                             | 27/60 [00:00<00:00, 48.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|████████████████████████████████████████████████▍                                  | 35/60 [00:00<00:00, 57.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 66.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                              | 4/60 [00:00<00:01, 38.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|███████████▏                                                                        | 8/60 [00:00<00:01, 38.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 12/60 [00:00<00:01, 36.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|███████████████████████▌                                                           | 17/60 [00:00<00:01, 39.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████████████████████▍                                                    | 22/60 [00:00<00:00, 42.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████████████████████████                                           | 29/60 [00:00<00:00, 50.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████████████████████████████████▏                               | 37/60 [00:00<00:00, 58.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 67.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                              | 4/60 [00:00<00:01, 39.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|████████████▌                                                                       | 9/60 [00:00<00:01, 40.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|███████████████████▎                                                               | 14/60 [00:00<00:01, 39.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████████████████▎                                                        | 19/60 [00:00<00:00, 42.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|███████████████████████████████████▉                                               | 26/60 [00:00<00:00, 50.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████████████████████████████████████████████▋                                     | 33/60 [00:00<00:00, 55.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|████████████████████████████████████████████████████████▋                          | 41/60 [00:00<00:00, 60.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 60.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "  0%|                                                                                           | 0/155 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/c/users/fahad/thesis/code/peal_private/peal/data/dataset_utils.py\u001b[0m(95)\u001b[0;36mparse_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     93 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfounding_factors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0mextract_instances_tensor_confounder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cfkd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'x_list': [torch.tensor([0.9983, 0.0577]), torch.tensor([0.6026, 0.7980]), torch.tensor([0.7544, 0.6564]), torch.tensor([-0.3068, -0.9518]), torch.tensor([0.4866, 0.8736]), torch.tensor([-0.4967, -0.8679]), torch.tensor([0.0115, 0.9999]), torch.tensor([0.9808, 0.1951]), torch.tensor([0.1724, 0.9850]), torch.tensor([-0.9552, -0.2958]), torch.tensor([0.9983, 0.0577]), torch.tensor([0.8502, 0.5264]), torch.tensor([-0.5264, -0.8502]), torch.tensor([-0.8792, -0.4765]), torch.tensor([-0.4039, -0.9148]), torch.tensor([-0.4663, -0.8846]), torch.tensor([0.6651, 0.7467]), torch.tensor([-0.4866, -0.8736]), torch.tensor([0.1837, 0.9830]), torch.tensor([0.9282, 0.3720]), torch.tensor([0.7910, 0.6118]), torch.tensor([0.9904, 0.1382]), torch.tensor([0.2064, 0.9785]), torch.tensor([0.4249, 0.9052]), torch.tensor([0.6026, 0.7980]), torch.tensor([0.9003, 0.4354]), torch.tensor([0.9366, 0.3504]), torch.tensor([0.6651, 0.7467]), torch.tensor([0.9052, 0.4249]), torch.tensor([0.1038, 0.9946]), torch.tensor([0.9967, 0.0808]), torch.tensor([-0.9933, -0.1152]), torch.tensor([-0.5166, -0.8562]), torch.tensor([0.8184, 0.5746]), torch.tensor([-0.3612, -0.9325]), torch.tensor([0.9760, 0.2177]), torch.tensor([0.7839, 0.6209]), torch.tensor([0.2402, 0.9707]), torch.tensor([-1.8370e-16, -1.0000e+00]), torch.tensor([0.8900, 0.4560]), torch.tensor([0.4866, 0.8736]), torch.tensor([-0.8679, -0.4967]), torch.tensor([0.7693, 0.6388]), torch.tensor([-0.0231, -0.9997]), torch.tensor([-0.2402, -0.9707]), torch.tensor([-0.9482, -0.3178]), torch.tensor([-0.9870, -0.1610]), torch.tensor([-0.1610, -0.9870]), torch.tensor([0.9785, 0.2064]), torch.tensor([0.9994, 0.0346]), torch.tensor([-0.9808, -0.1951]), torch.tensor([0.9850, 0.1724]), torch.tensor([-0.1382, -0.9904]), torch.tensor([0.3504, 0.9366]), torch.tensor([-0.4457, -0.8952]), torch.tensor([0.0231, 0.9997])],\n",
    "    \n",
    "    'x_counterfactual_list': [torch.tensor([-0.3278, -0.9271]), torch.tensor([ 0.6094, -0.7962]), torch.tensor([ 0.4023, -0.9154]), torch.tensor([ 0.8901, -0.4527]), torch.tensor([ 0.5666, -0.8311]), torch.tensor([ 0.8767, -0.4678]), torch.tensor([-0.7687,  0.6222]), torch.tensor([ 0.6639, -0.7749]), torch.tensor([-0.8234,  0.5644]), torch.tensor([ 0.9490, -0.3264]), torch.tensor([ 0.6660, -0.7403]), torch.tensor([ 0.6616, -0.7394]), torch.tensor([ 0.8022, -0.6140]), torch.tensor([ 0.7779, -0.6191]), torch.tensor([ 0.8159, -0.5691]), torch.tensor([ 0.7458, -0.6825]), torch.tensor([ 0.5259, -0.8478]), torch.tensor([ 0.7926, -0.6306]), torch.tensor([-0.8557,  0.5258]), torch.tensor([ 0.6206, -0.7707]), torch.tensor([-0.7720,  0.6367]), torch.tensor([ 0.3622, -0.9374]), torch.tensor([-0.8194,  0.5773]), torch.tensor([ 0.5147, -0.8544]), torch.tensor([-0.9915,  0.0854]), torch.tensor([ 0.6768, -0.7524]), torch.tensor([ 0.5410, -0.8401]), torch.tensor([-0.7550,  0.6432]), torch.tensor([ 0.5703, -0.8203]), torch.tensor([-0.9732, -0.2271]), torch.tensor([ 0.6819, -0.7523]), torch.tensor([-0.5816,  0.8087]), torch.tensor([ 0.8307, -0.5553]), torch.tensor([ 0.4141, -0.9123]), torch.tensor([ 0.7574, -0.6468]), torch.tensor([ 0.6505, -0.7626]), torch.tensor([ 0.6310, -0.7757]), torch.tensor([-0.9183,  0.3827]), torch.tensor([ 0.7822, -0.6366]), torch.tensor([ 0.3387, -0.9406]), torch.tensor([-0.3059,  0.1437]), torch.tensor([-0.6234,  0.7672]), torch.tensor([-0.8768,  0.4734]), torch.tensor([ 0.8038, -0.6209]), torch.tensor([ 0.7816, -0.6295]), torch.tensor([ 0.7685, -0.6677]), torch.tensor([-0.6739,  0.7387]), torch.tensor([ 0.6678, -0.7440]), torch.tensor([-0.8223,  0.5694]), torch.tensor([ 0.5737, -0.8168]), torch.tensor([ 0.4579, -0.8889]), torch.tensor([ 0.5802, -0.8145]), torch.tensor([ 0.8193, -0.5774]), torch.tensor([ 0.7992, -0.6015]), torch.tensor([ 0.9552, -0.2958])],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor_config.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "for idx, key in enumerate(dataset.data):\n",
    "    data[idx] = dataset.data[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_out = {\n",
    "    'x_list': [torch.tensor([-0.9983, -0.0577]), torch.tensor([-0.9239, -0.3827]), torch.tensor([0.2625, 0.9649]), torch.tensor([-0.2290, -0.9734]), torch.tensor([-0.4249, -0.9052]), torch.tensor([-0.1152, -0.9933]), torch.tensor([0.4967, 0.8679]), torch.tensor([-0.8621, -0.5067]), torch.tensor([0.9997, 0.0231]), torch.tensor([-0.2064, -0.9785])],\n",
    "    'y_list': torch.tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0.]),\n",
    "    'y_source_list': torch.tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0]),\n",
    "    'y_target_list': torch.tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1]),\n",
    "    'y_target_start_confidence_list': torch.tensor([6.6621e-05, 4.2620e-05, 4.5146e-05, 6.3567e-05, 4.7831e-05, 7.9660e-05, 3.2440e-05, 3.9745e-05, 8.9317e-05, 6.6160e-05]),\n",
    "    'x_counterfactual_list': [torch.tensor([-0.4479, 0.8914]), torch.tensor([-0.8168, -0.5625]), torch.tensor([-0.7237, 0.6845]), torch.tensor([0.7815, -0.6335]), torch.tensor([0.8844, -0.4574]), torch.tensor([0.9444, -0.3056]), torch.tensor([-0.7455, 0.6578]), torch.tensor([-0.6356, 0.7643]), torch.tensor([0.5736, -0.8212]), torch.tensor([0.8232, -0.5641])],\n",
    "    'z_difference_list': torch.tensor([[-0.5504, -0.9491], [-0.1071, 0.1798], [0.9863, 0.2805], [-1.0104, -0.3399], [-1.3093, -0.4478], [-1.0596, -0.6878], [1.2421, 0.2102], [-0.2266, -1.2709], [0.4261, 0.8443], [-1.0296, -0.4143]]),\n",
    "    'y_target_end_confidence_list': [torch.tensor(0.9810), torch.tensor(4.1007e-05), torch.tensor(0.6721), torch.tensor(0.8129), torch.tensor(0.9863), torch.tensor(0.9983), torch.tensor(0.7672), torch.tensor(0.7127), torch.tensor(0.9163), torch.tensor(0.9303)],\n",
    "    'x_attribution_list': [torch.tensor([-0.9983, -0.0577]), torch.tensor([-0.9239, -0.3827]), torch.tensor([0.2625, 0.9649]), torch.tensor([-0.2290, -0.9734]), torch.tensor([-0.4249, -0.9052]), torch.tensor([-0.1152, -0.9933]), torch.tensor([0.4967, 0.8679]), torch.tensor([-0.8621, -0.5067]), torch.tensor([0.9997, 0.0231]), torch.tensor([-0.2064, -0.9785])],\n",
    "    'collage_path_list': ['peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000000', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000001', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000002', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000003', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000004', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000005', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000006', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000007', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000008', 'peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100/0/validation_collages/0000009']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = torch.stack(data['x_list'])\n",
    "x_counterfactual_list = torch.stack(data['x_counterfactual_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.7451, 0.0000],\n",
    "        [0.0000, 0.5355],\n",
    "        [0.0000, 0.8923],\n",
    "        [0.0000, 0.3088],\n",
    "        [0.0000, 0.7999],\n",
    "        [0.0000, 0.2300],\n",
    "        [0.5908, 0.0000],\n",
    "        [0.4921, 0.0000],\n",
    "        [0.5737, 0.0000],\n",
    "        [0.7567, 0.0000]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for i in range(10):\n",
    "    x, y = cfkd.dataloader_mixer.sample()\n",
    "    #if x.shape[0] == 100:\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.stack([tensor for i in range(len(xs)) for tensor in xs[i]])\n",
    "ys = torch.stack([tensor for i in range(len(ys)) for tensor in ys[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(data.data[:,0], data.data[:,1], c='lightgray')\n",
    "#plt.scatter(xs[:,0], xs[:,1], c=ys)\n",
    "#plt.scatter(x_counterfactual_list[:,0], x_counterfactual_list[:,1], color='green')\n",
    "#plt.scatter(x_list[:, 0], x_list[:, 1], color='red')\n",
    "plt.scatter(x[:,0], x[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_set, batch_size=3, shuffle=True)\n",
    "sample, _ = next(iter(test_dl))\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.scatter(sample[:,0], sample[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor_config.generator['grad_scales'] = [1.0]\n",
    "adaptor_config.generator['noise_steps_for_counterfactuals'] = [40]\n",
    "#adaptor_config.generator['num_iterations'] = 3\n",
    "target_classes = student(sample[:,:2]).argmin(dim=-1)\n",
    "list_counterfactuals, diff_latent, y_target_end_confidence, x_list = adaptor.edit(x_in=sample[:,:2], target_confidence_goal=0.9, target_classes=target_classes, classifier=student)\n",
    "\n",
    "input_idx = [0,1]\n",
    "xx1, xx2 = np.meshgrid(*[np.linspace(float(adaptor.data[:, [input_idx]].min()-0.5),float(adaptor.data[:, [input_idx]].max()+0.5), 500) for idx in input_idx])\n",
    "grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "z = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, z, levels=[0],linestyles='dashed', label='decision boundary')\n",
    "adaptor.plot_counterfactuals()\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.nn.Parameter(grid.clone())  # Assuming 2 input features\n",
    "\n",
    "# Compute logits\n",
    "logits = student(input_data)\n",
    "\n",
    "# Compute gradients of the logits with respect to the inputs\n",
    "input_data.requires_grad = True\n",
    "logits[:,0].sum().backward(retain_graph=True)\n",
    "#logits[:,0].backward(torch.ones_like(logits), retain_graph=True)\n",
    "input_gradients = input_data.grad\n",
    "#input_data.grad.zero_()\n",
    "plt.quiver(input_data[:,0].detach(), input_data[:,1].detach(), input_gradients[:, 0], input_gradients[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 7)) \n",
    "\n",
    "for i in range(2):\n",
    "    input_data = torch.nn.Parameter(grid.clone())  # Assuming 2 input features\n",
    "    logits = student(input_data)\n",
    "    input_data.requires_grad = True\n",
    "    logits[:,i].sum().backward()\n",
    "    input_gradients = input_data.grad\n",
    "    axs[i].quiver(input_data[:,0].detach(), input_data[:,1].detach(), input_gradients[:, 0], input_gradients[:, 1])\n",
    "    input_data.grad.zero_()\n",
    "    axs[i].set_title(f'Class:{i}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 7)) \n",
    "\n",
    "for i in range(2):\n",
    "    input_data = torch.nn.Parameter(grid.clone())  # Assuming 2 input features\n",
    "    logits = fine_tuned(input_data)\n",
    "    input_data.requires_grad = True\n",
    "    logits[:,i].sum().backward()\n",
    "    input_gradients = input_data.grad\n",
    "    axs[i].quiver(input_data[:,0].detach(), input_data[:,1].detach(), input_gradients[:, 0], input_gradients[:, 1])\n",
    "    input_data.grad.zero_()\n",
    "    axs[i].set_title(f'Class:{i}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_classes = student(grid).argmax(dim=-1)\n",
    "#classifier_criterion = lambda x: F.cross_entropy(student(x), target_classes)\n",
    "sample_copy = torch.nn.Parameter(grid)\n",
    "#loss = classifier_criterion(sample_copy)\n",
    "#loss.backward()\n",
    "y = student(sample_copy)\n",
    "grad = torch.autograd.grad(outputs=y, inputs=sample_copy, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "plt.quiver(sample_copy[:,0].detach(), sample_copy[:,1].detach(), grad[:,0].detach(), grad[:,1].detach())\n",
    "xx1, xx2 = np.meshgrid(*[np.linspace(float(adaptor.data[:, [input_idx]].min()-0.5),float(adaptor.data[:, [input_idx]].max()+0.5), 500) for idx in input_idx])\n",
    "grid2 = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "z = student(grid2).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, z, levels=[0],linestyles='dashed', label='decision boundary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(data.data[:,0], data.data[:,1], c='lightgray')#c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "\n",
    "step = 5\n",
    "point = adaptor.counterfactuals_series[1]\n",
    "for j in range(0, point.shape[0]-step, step): # jth counterfactual\n",
    "    plt.arrow(\n",
    "        point[j, 0], point[j, 1], # plot the original point plus arrow until (j+granularity)th point\n",
    "        point[j+step, 0] - point[j, 0], \n",
    "        point[j+step, 1] - point[j, 1],\n",
    "        head_width=0.09, head_length=0.09, fc='blue', ec='blue'\n",
    "    )\n",
    "    plt.scatter(point[j+step, 0], point[j+step, 1], color='red', s=15)\n",
    "plt.scatter(point[0, 0], point[0, 1], color='black', label='start')\n",
    "plt.arrow(point[j, 0], point[j, 1],\n",
    "              point[-1, 0] - point[j, 0],\n",
    "              point[-1, 1] - point[j, 1],\n",
    "              head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "#input_idx = [0,1]\n",
    "#xx1, xx2 = np.meshgrid(*[np.linspace(float(adaptor.data[:, [input_idx]].min()-0.5),float(adaptor.data[:, [input_idx]].max()+0.5), 1000) for idx in input_idx])\n",
    "#grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "#z = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, z, levels=[0],linestyles='dashed', label='decision boundary')\n",
    "plt.scatter(point[-1, 0], point[-1, 1], color='lime')\n",
    "plt.scatter(point[-1, 0], point[-1, 1], color='lime')\n",
    "plt.scatter(point[-1, 0], point[-1, 1], color='cyan', label='end')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], color='lightgray', s=10)\n",
    "input_idx = [0,1]\n",
    "xx1, xx2 = np.meshgrid(*[np.linspace(float(data[:, [input_idx]].min()-0.5),float(data[:, [input_idx]].max()+0.5), 1000) for idx in input_idx])\n",
    "grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "plt.contour(xx1, xx2, z, levels=[0],linestyles='dashed', label='decision boundary')\n",
    "\n",
    "for i in range(0, len(list_counterfactuals)):\n",
    "    color = cmap(i / len(list_counterfactuals))  # Gradual color change\n",
    "    plt.scatter(list_counterfactuals[i][0], list_counterfactuals[i][1], c=color, s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_classes = student(sample[:,:2]).softmax(dim=-1).argmin(dim=-1)\n",
    "def plot_counterfactuals(counterfactuals, unguided_grads, guided_grads, pointwise_evolution=None, granularity=2):\n",
    "    #stack = torch.stack(counterfactuals).permute(1, 0, 2) \n",
    "    \n",
    "    bs = len(counterfactuals)\n",
    "    #nrows = np.ceil(np.sqrt(bs))\n",
    "    #fig, axs = plt.subplots(nrows=int(nrows), ncols=np.ceil(bs/nrows), figsize=(20, 20))\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    \n",
    "    plt.scatter(data.data[:,0], data.data[:,1], c='lightcyan')#c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "    \n",
    "    for i, point in enumerate(counterfactuals):\n",
    "        skip = point.shape[0]//granularity\n",
    "        #skip = 1\n",
    "        for j in range(0, point.shape[0] - skip, skip): # jth counterfactual\n",
    "            step = j+skip\n",
    "            # plot guided gradient at the last point to check direction (even though computer only for the first point)\n",
    "            plt.arrow(point[j, 0], point[j, 1], 5.0*guided_grads[i][j][0], guided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "\n",
    "            # plot the diffusion gradient\n",
    "            plt.arrow(point[j, 0], point[j, 1], 50.0*unguided_grads[i][j][0], unguided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "            \n",
    "            if step>=point.shape[0]:\n",
    "                break\n",
    "                step = point.shape[0]\n",
    "                \n",
    "            plt.arrow(\n",
    "                point[j, 0], point[j, 1], # plot the original point plus arrow until (j+granularity)th point\n",
    "                point[step, 0] - point[j, 0], \n",
    "                point[step, 1] - point[j, 1],\n",
    "                head_width=0.05, head_length=0.05, fc='blue', ec='blue'\n",
    "            ) \n",
    "            plt.scatter(point[step, 0], point[step, 1], color='black')\n",
    "        \n",
    "        #if j+skip <point.shape[0]:\n",
    "        \n",
    "        if j+skip >= point.shape[0]:\n",
    "            plt.arrow(point[j, 0], point[j, 1],\n",
    "                          point[-1, 0] - point[j, 0],\n",
    "                          point[-1, 1] - point[j, 1],\n",
    "                          head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "            #plt.arrow(point[j, 0], point[j, 1], 10.0*guided_grads[i][j][0], guided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "\n",
    "            # plot the diffusion gradient\n",
    "            #plt.arrow(point[j, 0], point[j, 1], 10.0*unguided_grads[i][j][0], unguided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "\n",
    "         \n",
    "        else:\n",
    "            plt.arrow(point[j+skip, 0], point[j+skip, 1],\n",
    "                          point[-1, 0] - point[j+skip, 0],\n",
    "                          point[-1, 1] - point[j+skip, 1],\n",
    "                          head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "            plt.arrow(point[j+skip, 0], point[j+skip, 1], guided_grads[i][j+skip][0], guided_grads[i][j+skip][1], head_width=0.08, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "\n",
    "            # plot the diffusion gradient\n",
    "            plt.arrow(point[j+skip, 0], point[j+skip,1], unguided_grads[i][j+skip][0], unguided_grads[i][j+skip][1], head_width=0.08, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "          \n",
    "            \n",
    "        # plot the last counterfactual\n",
    "        #plt.arrow(point[j, 0], point[j, 1],\n",
    "        #                  point[-1, 0] - point[j, 0],\n",
    "        #                  point[-1, 1] - point[j, 1],\n",
    "        #                  head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "\n",
    "        \n",
    "        plt.scatter(point[0, 0], point[0, 1], color='lime')#, label='start')\n",
    "        plt.scatter(point[-1, 0], point[-1, 1], color='red')#, label='end')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#counterfactuals, guided_grads, unguided_grads, total_series = adaptor.sample_counterfactual_ddpm(clean_batch=sample[6:7,:2], model=adaptor.model, classifier=student, num_noise_steps=60, target_classes=target_classes[6:7], classifier_grad_weight=0.5)\n",
    "plot_counterfactuals(adaptor.counterfactuals_series, adaptor.unguided_grads, adaptor.guided_grads)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(data.data[:,0], data.data[:,1], c='lightcyan')#c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "\n",
    "step = 1\n",
    "point = adaptor.counterfactuals_series[0]\n",
    "for j in range(0, point.shape[0]-step, step): # jth counterfactual\n",
    "    plt.arrow(\n",
    "        point[j, 0], point[j, 1], # plot the original point plus arrow until (j+granularity)th point\n",
    "        point[j+step, 0] - point[j, 0], \n",
    "        point[j+step, 1] - point[j, 1],\n",
    "        head_width=0.05, head_length=0.05, fc='blue', ec='blue'\n",
    "    ) \n",
    "    plt.scatter(point[j+step, 0], point[j+step, 1], color='black')\n",
    "    plt.scatter(point[0, 0], point[0, 1], color='lime')#, label='start')\n",
    "    plt.scatter(point[-1, 0], point[-1, 1], color='red')#, label='end')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting different series for the same point\n",
    "point_no = 1\n",
    "\n",
    "for num_series in [20, 25]:#, 32, 33, 40, 50, 78]: #range(0, len(total_series), len(total_series) // 4):  # only selecting some counterfactuals\n",
    "    series = total_series[num_series]  # selecting a counterfactual evolution series for one point\n",
    "    stack = torch.stack(series)\n",
    "    first_points = stack[:, point_no, :] # second index for different points # we are only taking the first point as above\n",
    "\n",
    "    skip = len(first_points) // 10 # set steps to skip in each series\n",
    "    if skip == 0:\n",
    "        skip = 1\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(data.data[:,0], data.data[:,1])#, c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "    plt.scatter(first_points[0, 0], first_points[0, 1], color='lime', label='start')\n",
    "    #plt.arrow(first_points[0, 0], first_points[0, 1],\n",
    "    #              first_points[25, 0] - first_points[0, 0],\n",
    "    #              first_points[25, 1] - first_points[0, 1],\n",
    "    #              head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "       \n",
    "    for i in range(0, len(stack)-skip, skip):\n",
    "        #plt.scatter(first_points[i, 0], first_points[i, 1], color='black')\n",
    "        plt.arrow(first_points[i, 0], first_points[i, 1],\n",
    "                  first_points[i+skip, 0] - first_points[i, 0],\n",
    "                  first_points[i+skip, 1] - first_points[i, 1],\n",
    "                  head_width=0.00005, head_length=0.000005, fc='blue', ec='blue')\n",
    "        plt.scatter(first_points[i+skip, 0], first_points[i+skip, 1], color='black')\n",
    "        \n",
    "\n",
    "\n",
    "    plt.arrow(first_points[i+skip, 0], first_points[i+skip, 1],\n",
    "                  first_points[-1, 0] - first_points[i+skip, 0],\n",
    "                  first_points[-1, 1] - first_points[i+skip, 1],\n",
    "                  head_width=0.000005, head_length=0.000005, fc='blue', ec='blue')\n",
    "    plt.arrow(first_points[0, 0], first_points[0, 1], 10*guided_grads[point_no][num_series][0], 10*guided_grads[point_no][num_series][1], head_width=0.05, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "    plt.arrow(first_points[0, 0], first_points[0, 1], 10*unguided_grads[point_no][num_series][0], 10*unguided_grads[point_no][num_series][1], head_width=0.05, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "\n",
    "    plt.scatter(first_points[-1, 0], first_points[-1, 1], color='red', label='end')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(f'Timestep: {79 - num_series}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T09:15:10.925581Z",
     "start_time": "2023-08-31T09:15:10.924371100Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_100_grad_1/32/finetuned_model/model.cpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = fine_tuned(grid).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_actual = contour.gather(1, contour.argmax(axis=-1).unsqueeze(1))\n",
    "contour_actual_min = contour.gather(1, contour.argmin(axis=-1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1_span = np.linspace(-1.1, 1.1, 1000)\n",
    "#x2_span = np.linspace(-1.1, 1.1, 1000)\n",
    "#xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "#grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "#fine_tuned.eval()\n",
    "#z = fine_tuned(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "z1 = contour_actual.reshape(xx1.shape)\n",
    "z2 = z = contour_actual_min.reshape(xx1.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#a = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "#b = teacher(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "\n",
    "\n",
    "#data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "#for idx, key in enumerate(dataset.data):\n",
    "#    data[idx] = dataset.data[key]\n",
    "ax.scatter(data[:,0], data[:,1], c=np.where(data[:,-1] == 0, 'lightcyan', 'lightgray')[0])\n",
    "ax.contour(xx1, xx2, z1, levels=[0.5], lcmap='coolwarm', label=f'confidence: 0.5')\n",
    "ax.contour(xx1, xx2, z1, levels=[0.51], lcmap='coolwarm', label=f'confidence: 0.51')\n",
    "ax.contour(xx1, xx2, z1, levels=[0.515], lcmap='coolwarm', label=f'confidence: 0.515')\n",
    "ax.contour(xx1, xx2, z1, levels=[0.52], lcmap='coolwarm', label=f'confidence: 0.52')\n",
    "ax.contour(xx1, xx2, z1, levels=[0.534], lcmap='coolwarm', label=f'confidence: 0.534')\n",
    "ax.contour(xx1, xx2, z1, levels=[0.55], lcmap='coolwarm', label=f'confidence: 0.55')\n",
    "ax.contour(xx1, xx2, z2, levels=[0.49], lcmap='coolwarm', label=f'confidence: 0.49')\n",
    "ax.contour(xx1, xx2, z2, levels=[0.3], lcmap='coolwarm', label=f'confidence: 0.49')\n",
    "#ax.contour(xx1, xx2, z, levels=[0], lcmap='coolwarm')\n",
    "#ax.contour(xx1, xx2, z, levels=[-0.05], lcmap='coolwarm')\n",
    "#ax.contour(xx1, xx2, z, levels=[-0.1], lcmap='coolwarm')\n",
    "#ax.contour(xx1, xx2, z, levels=[-1], lcmap='coolwarm')\n",
    "ax.contour(xx1, xx2, a, levels=[0], linestyles='dashed', colors='red', label='student')\n",
    "ax.contour(xx1, xx2, b, levels=[0], linestyles='dashed', colors='green', label='teacher')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = fine_tuned(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_symbolic_100_classifier/cfkd_ddpm_oracle_50/model.cpl')\n",
    "sum(fine_tuned(torch.tensor(data.data)).softmax(dim=-1).argmax(dim=-1) == torch.tensor(data.label)) / len(data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_20_grad_1/model.cpl')\n",
    "x1_span = np.linspace(-1.1, 1.1, 1000)\n",
    "x2_span = np.linspace(-1.1, 1.1, 1000)\n",
    "xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "fine_tuned.eval()\n",
    "z = fine_tuned(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "fig, ax = plt.subplots()\n",
    "a = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "b = teacher(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "\n",
    "\n",
    "#data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "#for idx, key in enumerate(dataset.data):\n",
    "#    data[idx] = dataset.data[key]\n",
    "ax.scatter(data[:,0], data[:,1], c=np.where(data[:,-1] == 0, 'lightcyan', 'lightgray')[0])\n",
    "ax.contour(xx1, xx2, z, levels=[0], linestyles='dashed', label='fine-tuned')\n",
    "\n",
    "ax.contour(xx1, xx2, a, levels=[0], linestyles='dashed', colors='red', label='student')\n",
    "ax.contour(xx1, xx2, b, levels=[0], linestyles='dashed', colors='green', label='teacher')\n",
    "plt.title('20 Iterations')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_symbolic_100_classifier/cfkd_ddpm_oracle_100/model.cpl')\n",
    "sum(fine_tuned(torch.tensor(data.data)).softmax(dim=-1).argmax(dim=-1) == torch.tensor(data.label)) / len(data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_30_grad_1/model.cpl')\n",
    "#x1_span = np.linspace(-1.1, 1.1, 1000)\n",
    "#x2_span = np.linspace(-1.1, 1.1, 1000)\n",
    "#xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "#grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "fine_tuned.eval()\n",
    "z = fine_tuned(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "fig, ax = plt.subplots()\n",
    "a = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "b = teacher(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "\n",
    "\n",
    "#data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "#for idx, key in enumerate(dataset.data):\n",
    "#    data[idx] = dataset.data[key]\n",
    "ax.scatter(data[:,0], data[:,1], c=np.where(data[:,-1] == 0, 'lightcyan', 'lightgray')[0])\n",
    "ax.contour(xx1, xx2, z, levels=[0], linestyles='dashed', label='fine-tuned')\n",
    "\n",
    "ax.contour(xx1, xx2, a, levels=[0], linestyles='dashed', colors='red', label='student')\n",
    "ax.contour(xx1, xx2, b, levels=[0], linestyles='dashed', colors='green', label='teacher')\n",
    "plt.title('10 Iterations')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_50_grad_1/model.cpl')\n",
    "#x1_span = np.linspace(-1.1, 1.1, 1000)\n",
    "#x2_span = np.linspace(-1.1, 1.1, 1000)\n",
    "#xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "#grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "fine_tuned.eval()\n",
    "z = fine_tuned(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "fig, ax = plt.subplots()\n",
    "a = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "b = teacher(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "\n",
    "\n",
    "#data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "#for idx, key in enumerate(dataset.data):\n",
    "#    data[idx] = dataset.data[key]\n",
    "ax.scatter(data[:,0], data[:,1], c=np.where(data[:,-1] == 0, 'lightcyan', 'lightgray')[0])\n",
    "ax.contour(xx1, xx2, z, levels=[0], linestyles='dashed', label='fine-tuned')\n",
    "\n",
    "ax.contour(xx1, xx2, a, levels=[0], linestyles='dashed', colors='red', label='student')\n",
    "ax.contour(xx1, xx2, b, levels=[0], linestyles='dashed', colors='green', label='teacher')\n",
    "plt.title('10 Iterations')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_circle_poisened_classifier/cfkd_ddpm_oracle_steps_50_grad_1/model.cpl')\n",
    "x1_span = np.linspace(-1.1, 1.1, 1000)\n",
    "x2_span = np.linspace(-1.1, 1.1, 1000)\n",
    "xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "fine_tuned.eval()\n",
    "z = fine_tuned(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "fig, ax = plt.subplots()\n",
    "a = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "b = teacher(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "\n",
    "\n",
    "data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "for idx, key in enumerate(dataset.data):\n",
    "    data[idx] = dataset.data[key]\n",
    "ax.scatter(data[:,0], data[:,1], c=np.where(data[:,-1] == 0, 'lightcyan', 'lightgray')[0])\n",
    "ax.contour(xx1, xx2, z, levels=[0], linestyles='dashed', label='fine-tuned')\n",
    "\n",
    "ax.contour(xx1, xx2, a, levels=[0], linestyles='dashed', colors='red', label='student')\n",
    "ax.contour(xx1, xx2, b, levels=[0], linestyles='dashed', colors='green', label='teacher')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = torch.load('peal_runs/artificial_symbolic_100_classifier/cfkd_ddpm_new_oracle_100/model.cpl')\n",
    "x1_span = np.linspace(-1.1, 1.1, 1000)\n",
    "x2_span = np.linspace(-1.1, 1.1, 1000)\n",
    "xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "fine_tuned.eval()\n",
    "z = fine_tuned(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "fig, ax = plt.subplots()\n",
    "a = student(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "b = teacher(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "\n",
    "\n",
    "data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "for idx, key in enumerate(dataset.data):\n",
    "    data[idx] = dataset.data[key]\n",
    "ax.scatter(data[:,0], data[:,1], c=np.where(data[:,-1] == 0, 'lightcyan', 'lightgray')[0])\n",
    "ax.contour(xx1, xx2, z, levels=[0], linestyles='dashed', label='fine-tuned')\n",
    "\n",
    "ax.contour(xx1, xx2, a, levels=[0], linestyles='dashed', colors='red', label='student')\n",
    "ax.contour(xx1, xx2, b, levels=[0], linestyles='dashed', colors='green', label='teacher')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, num_timesteps: int, var_schedule='linear'):\n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        def linear_schedule(num_timesteps: int):\n",
    "            scale = 1000 / num_timesteps\n",
    "            min_var = scale * 1e-5\n",
    "            max_var = scale * 1e-2\n",
    "            return torch.linspace(min_var, max_var, num_timesteps, dtype=torch.float32)\n",
    "\n",
    "        def cosine_schedule(num_timesteps, s=0.008):\n",
    "            steps = num_timesteps + 1\n",
    "            x = torch.linspace(0, num_timesteps, steps, dtype=torch.float64)\n",
    "            alphas_cumprod = torch.cos(((x / num_timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "            return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "        if var_schedule == 'linear':\n",
    "            betas = linear_schedule(num_timesteps)\n",
    "\n",
    "        if var_schedule == 'cosine':\n",
    "            betas = cosine_schedule(num_timesteps)\n",
    "\n",
    "        self.register_buffer(\"beta\", betas)\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(0))\n",
    "\n",
    "    def forward_diffusion(self, clean_x: torch.Tensor, noise: torch.tensor, timestep: torch.Tensor):\n",
    "        timestep = torch.tensor([timestep])\n",
    "        if timestep.shape[0] == clean_x.shape[0]:\n",
    "            alpha_bar_t = self.alpha_bar[timestep][:, None]\n",
    "        else:\n",
    "            alpha_bar_t = self.alpha_bar[timestep].repeat(clean_x.shape[0])[:, None]\n",
    "        mu = torch.sqrt(alpha_bar_t)\n",
    "        std = torch.sqrt(1 - alpha_bar_t)\n",
    "        noisy_x = mu * clean_x + std * noise\n",
    "        return noisy_x\n",
    "    \n",
    "    \n",
    "    def forward_diffusion_ddim():\n",
    "        pass\n",
    "\n",
    "    def loss(self, model: nn.Module, clean_x: torch.Tensor, loss='L1_simple', var_model: nn.Module=None) -> torch.Tensor:\n",
    "        t = torch.randint(self.num_timesteps, (clean_x.shape[0],))\n",
    "        #t = int(torch.rand(1)*self.num_timesteps)\n",
    "        eps_t = torch.randn_like(clean_x)\n",
    "        alpha_bar_t = self.alpha_bar[t][:, None]\n",
    "        \n",
    "        #x_t = self.forward_diffusion(clean_x=clean_x, noise=eps_t, timestep=t)\n",
    "        \n",
    "        x_t = torch.sqrt(alpha_bar_t) * clean_x + torch.sqrt(1 - alpha_bar_t) * eps_t\n",
    "        \n",
    "        eps_hat = model(x=x_t, t=t)\n",
    "        if loss == \"L1_simple\":\n",
    "              loss_diff = nn.MSELoss(reduction='sum')(eps_hat, eps_t)\n",
    "\n",
    "        return loss_diff\n",
    "    \n",
    "\n",
    "    def reverse_diffusion_ddpm(self, noisy_x: torch.Tensor, model: nn.Module, timestep: torch.Tensor):\n",
    "        alpha_t = self.alpha[timestep].repeat(noisy_x.shape[0])[:, None]\n",
    "        alpha_bar_t = self.alpha_bar[timestep].repeat(noisy_x.shape[0])[:, None]\n",
    "        beta_t = 1 - alpha_t\n",
    "        eps_hat = model(x=noisy_x, t=timestep)\n",
    "        posterior_mean = (1 / torch.sqrt(alpha_t)) * (noisy_x - (beta_t / torch.sqrt(1 - alpha_bar_t) * eps_hat))\n",
    "        z = torch.randn_like(noisy_x)\n",
    "        \n",
    "        if timestep > 0:\n",
    "            denoised_x = posterior_mean + torch.sqrt(beta_t)*z #* z * (timestep > 0))  # variance = beta_t\n",
    "        else:\n",
    "            denoised_x = posterior_mean\n",
    "                                           \n",
    "        return denoised_x\n",
    "    \n",
    "    def reverse_diffusion_ddim():\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample_ddpm(self, model: nn.Module, n_samples: int = 256, label=None):\n",
    "        x_pred = []\n",
    "        x = torch.randn(n_samples, self.input_dim)\n",
    "        x_pred.append(x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in reversed(range(0, self.num_timesteps)):\n",
    "                #alpha_t = self.alpha[t].repeat(n_samples)[:, None]\n",
    "                #alpha_bar_t = self.alpha_bar[t].repeat(n_samples)[:, None]\n",
    "                #eps_hat = model(x, t)\n",
    "\n",
    "                #sigma_t = torch.sqrt(1 - self.alpha[t])\n",
    "                #x_t_mean = (x - eps_hat * (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) / torch.sqrt(alpha_t)\n",
    "\n",
    "                #if t > 0:\n",
    "                #    x = x_t_mean + (sigma_t * torch.randn_like(x))\n",
    "                #else:\n",
    "                #    x = x_t_mean\n",
    "                x = self.reverse_diffusion_ddpm(noisy_x=x, model=model, timestep=t)\n",
    "                \n",
    "                x_pred.append(x)\n",
    "                \n",
    "        return x_pred\n",
    "\n",
    "\n",
    "    def sample_counterfactual_ddpm(self, clean_batch: torch.Tensor, model: nn.Module, classifier: nn.Module, num_noise_steps: int, counterfactual_class: torch.Tensor, classifier_grad_weight: float, perceptual_weight: float):\n",
    "        \n",
    "        classifier.eval()\n",
    "        \n",
    "        # DEFINE BATCH SIZE AND COUNTERFACTUAL CLASS\n",
    "        bs = clean_batch.shape[0]\n",
    "        #label = torch.tensor(counterfactual_class).repeat(clean_batch.shape[0])\n",
    "        \n",
    "        # COMPUTE CLEAN GRADIENTS FOR THE FIRST STEP\n",
    "        #print(classifier(clean_batch).softmax(dim=-1).argmax(dim=-1))\n",
    "        classifier_criterion = lambda x: F.cross_entropy(classifier(x), counterfactual_class)\n",
    "        clean_batch_copy = torch.nn.Parameter(clean_batch)\n",
    "        loss = classifier_criterion(clean_batch_copy)\n",
    "        loss.backward()\n",
    "        \n",
    "        clean_grad = classifier_grad_weight * clean_batch_copy.grad.detach()\n",
    "        \n",
    "        # REDEFINING VARIABLES AND PERFORMING FORWARD DIFFUSION\n",
    "        #next_z = clean_batch\n",
    "        eps_t = torch.randn_like(clean_batch)\n",
    "        \n",
    "        #next_z = self.forward_diffusion(clean_x=clean_batch, noise=eps_t, timestep=num_noise_steps)\n",
    " \n",
    "        alpha_bar_t = self.alpha_bar[num_noise_steps].repeat(bs)[:, None]\n",
    "        alpha_t = self.alpha[num_noise_steps].repeat(bs)[:, None]\n",
    "        next_z = torch.sqrt(alpha_bar_t) * clean_batch + torch.sqrt(1 - alpha_bar_t) * eps_t\n",
    "        \n",
    "        #plt.scatter(data.data[:,0], data.data[:,1], c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "        #plt.scatter(next_z[:,0], next_z[:,1])\n",
    "        \n",
    "        counterfactuals = [] # total counterfactuals\n",
    "        counterfactuals.append(clean_batch)\n",
    "        guided_grads = []  # guided grads at the first step\n",
    "        unconditional_grads = [] # diffusion grads at the first step \n",
    "        total_series = [] # contains evolution from noisy to cleaned instance for each data point\n",
    "        losses = []\n",
    "        losses.append(loss)\n",
    "        for i in tqdm(range(0, num_noise_steps)[::-1]):\n",
    "            \n",
    "            # Denoise z_t to create z_t-1 (next z)\n",
    "            \n",
    "            alpha_i = self.alpha[i].repeat(bs)[:, None]\n",
    "            alpha_bar_i = self.alpha_bar[i].repeat(bs)[:, None]\n",
    "            sigma_i = torch.sqrt(1 - self.alpha[i])\n",
    "            eps_hat = model(next_z, i)\n",
    "\n",
    "            # Unconditional mean\n",
    "            unconditional_grad = -eps_hat / torch.sqrt(1 - alpha_bar_i)\n",
    "            z_t_mean = (next_z + unconditional_grad * (1 - alpha_i)) / torch.sqrt(alpha_i)\n",
    "\n",
    "            # Guided mean\n",
    "            z_t_mean -= sigma_i * (clean_grad / torch.sqrt(alpha_bar_i))\n",
    "\n",
    "            if i > 0:\n",
    "                next_z = z_t_mean + (sigma_i * torch.randn_like(clean_batch)) \n",
    "            else:\n",
    "                next_z = z_t_mean\n",
    "\n",
    "            next_x = next_z.clone()\n",
    "            # Denoise to create a cleaned x (next x)\n",
    "            series = []\n",
    "            series.append(next_x.detach())\n",
    "            for t in range(0, i)[::-1]:\n",
    "                if i == 0:\n",
    "                    break\n",
    "                #next_x = self.reverse_diffusion_ddpm(noisy_x=next_x, model=model, timestep=t)\n",
    "                alpha_t = self.alpha[t].repeat(bs)[:, None]\n",
    "                alpha_bar_t = self.alpha_bar[t].repeat(bs)[:, None]\n",
    "                sigma_t = torch.sqrt(1 - self.alpha[t])\n",
    "                eps_hat = model(next_x, t)\n",
    "                next_x = (next_x - eps_hat * (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) / torch.sqrt(alpha_t)\n",
    "                if t > 0:\n",
    "                    next_x = next_x + (sigma_t * torch.randn_like(next_x))\n",
    "                else:\n",
    "                    next_x = next_x\n",
    "                \n",
    "                series.append(next_x.detach())\n",
    "            total_series.append(series)\n",
    "            guided_grads.append(-sigma_i * clean_grad.detach() / torch.sqrt(alpha_bar_i))\n",
    "            unconditional_grads.append(unconditional_grad.detach() * (1 - alpha_i) / torch.sqrt(alpha_i) )\n",
    "            \n",
    "            if i != 0:\n",
    "                counterfactuals.append(next_x.detach())\n",
    "\n",
    "\n",
    "            # Gradient wrt denoised image (next_x)\n",
    "            next_x_copy = torch.nn.Parameter(next_x.clone())\n",
    "            loss = classifier_criterion(next_x_copy)\n",
    "            loss.backward()\n",
    "            losses.append(loss)\n",
    "            clean_classifier_grad = next_x_copy.grad.detach()\n",
    "            radius = torch.sqrt((next_x**2).sum(axis=1))\n",
    "            perceptual_grad = (2 * (radius - 1))[:, None]\n",
    "            clean_grad = classifier_grad_weight * clean_classifier_grad + perceptual_weight * perceptual_grad\n",
    "            \n",
    "            self.counterfactuals = counterfactuals\n",
    "            self.guided_grads = guided_grads\n",
    "            self.diffusion_grads = unconditional_grads\n",
    "            self.pointwise_evolution = total_series \n",
    "            \n",
    "        counterfactuals = torch.stack(counterfactuals).permute(1, 0, 2) \n",
    "        guided_grads = torch.stack(guided_grads).permute(1, 0, 2) \n",
    "        unguided_grads = torch.stack(unconditional_grads).permute(1, 0, 2)\n",
    "        #total_series = torch.stack(total_series).permute(1, 0, 2)\n",
    "            \n",
    "        return counterfactuals, guided_grads, unguided_grads, total_series\n",
    "    \n",
    "\n",
    "    \n",
    "    def plot_counterfactuals():\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "embed_dim = 256\n",
    "T = 500\n",
    "\n",
    "model = BasicDiscreteTimeModel(input_dim, embed_dim, num_timesteps=T)\n",
    "ddpm = DDPM(input_dim=input_dim, embed_dim=embed_dim, num_timesteps=T)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.load_state_dict(torch.load('./peal_runs/artificial_symbolic_100_generator/ddpm.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ddpm.sample_ddpm(model, n_samples=500)\n",
    "plt.scatter(samples[-1][:,0], samples[-1][:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = student(sample[:,:2]).softmax(dim=-1).argmin(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals, guided_grads, unguided_grads, total_series = ddpm.sample_counterfactual_ddpm(clean_batch=sample[:,:2], model=model, classifier=student, num_noise_steps=60, counterfactual_class=target_classes, classifier_grad_weight=1.0, perceptual_weight=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CircleDataset(Dataset):\n",
    "    def __init__(self, data_path: str, features: list[str], labels: list[str]):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(data_path)\n",
    "        self.data = data[features].to_numpy('float32')\n",
    "        self.label = data[labels].to_numpy('long')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx,:], self.label[idx]\n",
    "    \n",
    "    def serialize_dataset(self, output_dir: str, x_list: list, y_list: list):\n",
    "        pass\n",
    "    \n",
    "    __name__ = 'circle'\n",
    "    \n",
    "data_config = {\n",
    "    'data_path': 'datasets/circle/size_500_radius_1_seed_0.csv',\n",
    "    'features': ['x1', 'x2'],\n",
    "    'target': 'Target'\n",
    "}\n",
    "\n",
    "data = CircleDataset(data_path=data_config['data_path'], features=data_config['features'], labels=data_config['target'])\n",
    "\n",
    "    \n",
    "def plot_counterfactuals(counterfactuals, diffusion_grads, classifier_grads, pointwise_evolution=None, granularity=2):\n",
    "    #stack = torch.stack(counterfactuals).permute(1, 0, 2) \n",
    "    \n",
    "    bs = len(counterfactuals)\n",
    "    #nrows = np.ceil(np.sqrt(bs))\n",
    "    #fig, axs = plt.subplots(nrows=int(nrows), ncols=np.ceil(bs/nrows), figsize=(20, 20))\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    \n",
    "    plt.scatter(data.data[:,0], data.data[:,1], c='lightcyan')#c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "    \n",
    "    for i, point in enumerate(counterfactuals):\n",
    "        skip = point.shape[0]//granularity\n",
    "        for j in range(0, point.shape[0] - skip, skip): # jth counterfactual\n",
    "            step = j+skip\n",
    "            # plot guided gradient at the last point to check direction (even though computer only for the first point)\n",
    "            plt.arrow(point[j, 0], point[j, 1], 5.0*guided_grads[i][j][0], 5.0*guided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "\n",
    "            # plot the diffusion gradient\n",
    "            plt.arrow(point[j, 0], point[j, 1], 20.0*unguided_grads[i][j][0], 20.0*unguided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "            \n",
    "            if step>=point.shape[0]:\n",
    "                break\n",
    "                step = point.shape[0]\n",
    "                \n",
    "            plt.arrow(\n",
    "                point[j, 0], point[j, 1], # plot the original point plus arrow until (j+granularity)th point\n",
    "                point[step, 0] - point[j, 0], \n",
    "                point[step, 1] - point[j, 1],\n",
    "                head_width=0.05, head_length=0.05, fc='blue', ec='blue'\n",
    "            ) \n",
    "            plt.scatter(point[step, 0], point[step, 1], color='black')\n",
    "        \n",
    "        #if j+skip <point.shape[0]:\n",
    "        \n",
    "        if j+skip >= point.shape[0]:\n",
    "            plt.arrow(point[j, 0], point[j, 1],\n",
    "                          point[-1, 0] - point[j, 0],\n",
    "                          point[-1, 1] - point[j, 1],\n",
    "                          head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "            plt.arrow(point[j, 0], point[j, 1], 10.0*guided_grads[i][j][0], 10.0*guided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "\n",
    "            # plot the diffusion gradient\n",
    "            plt.arrow(point[j, 0], point[j, 1], 50.0*unguided_grads[i][j][0], 50.0*unguided_grads[i][j][1], head_width=0.08, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "\n",
    "         \n",
    "        else:\n",
    "            plt.arrow(point[j+skip, 0], point[j+skip, 1],\n",
    "                          point[-1, 0] - point[j+skip, 0],\n",
    "                          point[-1, 1] - point[j+skip, 1],\n",
    "                          head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "            plt.arrow(point[j+skip, 0], point[j+skip, 1], 10.0*guided_grads[i][j+skip][0], 10.0*guided_grads[i][j+skip][1], head_width=0.08, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "\n",
    "            # plot the diffusion gradient\n",
    "            plt.arrow(point[j+skip, 0], point[j+skip,1], 50.0*unguided_grads[i][j+skip][0], 50.0*unguided_grads[i][j+skip][1], head_width=0.08, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "          \n",
    "            \n",
    "        # plot the last counterfactual\n",
    "        #plt.arrow(point[j, 0], point[j, 1],\n",
    "        #                  point[-1, 0] - point[j, 0],\n",
    "        #                  point[-1, 1] - point[j, 1],\n",
    "        #                  head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "\n",
    "        \n",
    "        plt.scatter(point[0, 0], point[0, 1], color='lime', label='start')\n",
    "        plt.scatter(point[-1, 0], point[-1, 1], color='red', label='end')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "counterfactuals, guided_grads, unguided_grads, total_series = adaptor.sample_counterfactual_ddpm(clean_batch=sample[:,:2], model=adaptor.model, classifier=student, num_noise_steps=80, target_classes=target_classes, classifier_grad_weight=15.0)\n",
    "plot_counterfactuals(counterfactuals[1:2,:], unguided_grads, guided_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_series[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting different series for the same point\n",
    "point_no = 1\n",
    "\n",
    "for num_series in [20, 25]:#, 32, 33, 40, 50, 78]: #range(0, len(total_series), len(total_series) // 4):  # only selecting some counterfactuals\n",
    "    series = total_series[num_series]  # selecting a counterfactual evolution series for one point\n",
    "    stack = torch.stack(series)\n",
    "    first_points = stack[:, point_no, :] # second index for different points # we are only taking the first point as above\n",
    "\n",
    "    skip = len(first_points) // 10 # set steps to skip in each series\n",
    "    if skip == 0:\n",
    "        skip = 1\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(data.data[:,0], data.data[:,1])#, c=np.where(data.label == 0, 'lightcyan', 'lightgray'))\n",
    "    plt.scatter(first_points[0, 0], first_points[0, 1], color='lime', label='start')\n",
    "    #plt.arrow(first_points[0, 0], first_points[0, 1],\n",
    "    #              first_points[25, 0] - first_points[0, 0],\n",
    "    #              first_points[25, 1] - first_points[0, 1],\n",
    "    #              head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "       \n",
    "    for i in range(0, len(stack)-skip, skip):\n",
    "        #plt.scatter(first_points[i, 0], first_points[i, 1], color='black')\n",
    "        plt.arrow(first_points[i, 0], first_points[i, 1],\n",
    "                  first_points[i+skip, 0] - first_points[i, 0],\n",
    "                  first_points[i+skip, 1] - first_points[i, 1],\n",
    "                  head_width=0.00005, head_length=0.000005, fc='blue', ec='blue')\n",
    "        plt.scatter(first_points[i+skip, 0], first_points[i+skip, 1], color='black')\n",
    "        \n",
    "\n",
    "\n",
    "    plt.arrow(first_points[i+skip, 0], first_points[i+skip, 1],\n",
    "                  first_points[-1, 0] - first_points[i+skip, 0],\n",
    "                  first_points[-1, 1] - first_points[i+skip, 1],\n",
    "                  head_width=0.000005, head_length=0.000005, fc='blue', ec='blue')\n",
    "    plt.arrow(first_points[0, 0], first_points[0, 1], 10*guided_grads[point_no][num_series][0], 10*guided_grads[point_no][num_series][1], head_width=0.05, head_length=0.05, fc='deeppink', ec='deeppink')\n",
    "    plt.arrow(first_points[0, 0], first_points[0, 1], 10*unguided_grads[point_no][num_series][0], 10*unguided_grads[point_no][num_series][1], head_width=0.05, head_length=0.05, fc='cadetblue', ec='cadetblue')\n",
    "\n",
    "    plt.scatter(first_points[-1, 0], first_points[-1, 1], color='red', label='end')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(f'Timestep: {79 - num_series}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 16, 20])\n",
    "y = np.array([0.7680, 0.8680, 0.9020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, marker='o')\n",
    "for i, (xi, yi) in enumerate(zip(x, y)):\n",
    "    plt.text(xi, yi, f'({yi})', fontsize=12, ha='right', va='bottom')\n",
    "\n",
    "plt.xlabel('No. of Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "def discard_counterfactuals(self, counterfactuals, classifier, target_classes, target_confidence, minimal_counterfactuals, tolerance=0.1):\n",
    "        \n",
    "        # compute distance of current minimal_counterefactuals from radius 1.0\n",
    "        current_counterfactual_distance_from_manifold = torch.abs((torch.pow(minimal_counterfactuals, 2).sum(dim=-1) - 1.0))\n",
    "        \n",
    "        for i in range(len(counterfactuals)):  \n",
    "\n",
    "            # compute classifier  for all the counterfactuals for each point\n",
    "            new_counterfactuals_confidence = classifier(counterfactuals[i]).softmax(dim=-1)[:, target_classes[i]]\n",
    "            \n",
    "            \n",
    "            # check if new counterfactuals satisfy the confidence constraint\n",
    "            new_confidence_satisfied = new_counterfactuals_confidence > target_confidence\n",
    "            \n",
    "            new_confidence_satisfied_indices = torch.nonzero(new_counterfactuals_confidence > target_confidence)\n",
    "            \n",
    "            \n",
    "            # \n",
    "            new_tolerance_satisfied = torch.abs((torch.pow(counterfactuals[i], 2).sum(dim=-1) - 1.0)) < tolerance\n",
    "            new_tolerance_satisfied_indices = torch.nonzero(torch.abs((torch.pow(counterfactuals[i], 2).sum(dim=-1) - 1.0)) < tolerance)\n",
    "            \n",
    "            # check where target confidence is reached AND if new counterfactuals are within the tolerance range\n",
    "            #indices = torch.nonzero((confidence > target_confidence) \n",
    "            #                    (torch.abs((torch.pow(counterfactuals[i], 2).sum(dim=-1) - 1.0)) < tolerance))\n",
    "            \n",
    "            \n",
    "            new_confidence_and_tolerance_satisfied_indices = torch.nonzero(new_confidence_satisfied & new_tolerance_satisfied)\n",
    "            \n",
    "            current_tolerance_satisfied = current_counterfactual_distance_from_manifold[i] < tolerance\n",
    "            #classifier(minimal_counterfactuals[i:i+1]).softmax(dim=-1)[0][target_classes[i]]\n",
    "            current_confidence_satisfied = classifier(minimal_counterfactuals[i:i+1]).softmax(dim=-1)[0][target_classes[i]].item() > target_confidence\n",
    "            \n",
    "            # if current counterfactual satisfies confidence and tolerance, maintain status quo \n",
    "            \n",
    "            \n",
    "            \n",
    "            if (current_tolerance_satisfied) and (current_confidence_satisfied):\n",
    "            \n",
    "                #print(f'confidence and tolerance satisfied for {i}')\n",
    "                continue\n",
    "          \n",
    "            # if new target confidence and current target confidence not satisfied\n",
    "            # but new tolerance is satisfied, then move the point closer\n",
    "            elif (new_tolerance_satisfied_indices.nelement() !=0) & (new_confidence_satisfied_indices.nelement() == 0) and (not current_confidence_satisfied):\n",
    "                minimal_counterfactuals[i] = counterfactuals[i][torch.nonzero(new_tolerance_satisfied)[-1].item()]\n",
    "            \n",
    "              \n",
    "            # if current counterfactual is on the manifold but is not actually a counterfactual, \n",
    "            # replace it with new counterfactual if there exists any \n",
    "            elif (not current_confidence_satisfied) & (new_confidence_and_tolerance_satisfied_indices.nelement() != 0):\n",
    "                # change this to only include the first where confidence and tolerance is satisfied\n",
    "                minimal_counterfactuals[i] = counterfactuals[i][new_confidence_and_tolerance_satisfied_indices[0].item()]\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "\n",
    "        return minimal_counterfactuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=500):\n",
    "\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        max_len += 1\n",
    "        self.P = torch.zeros(max_len, embed_dim)\n",
    "        freqs = torch.arange(max_len)[:, None] / (torch.pow(10000, torch.arange(0, embed_dim, 2, dtype=torch.float32)/embed_dim))\n",
    "\n",
    "        self.P[:,0::2] = torch.sin(freqs)\n",
    "        self.P[:,1::2] = torch.cos(freqs)\n",
    "        \n",
    "        self.P = self.P[1:]\n",
    "        \n",
    "    def forward(self, t):\n",
    "        return self.P[t]\n",
    "    \n",
    "class ScoreNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(ScoreNetwork, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.layer1 = nn.LazyLinear(embed_dim)\n",
    "        self.layer2 = nn.LazyLinear(embed_dim)\n",
    "        self.layer3 = nn.LazyLinear(embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.layer4 = nn.LazyLinear(input_dim)\n",
    "    \n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.layer1(x) + time_embed\n",
    "        x = F.silu(self.layer2(x))\n",
    "        x = F.silu(self.layer3(x))  \n",
    "        return self.layer4((self.norm(x)))\n",
    "                           \n",
    "class BasicDiscreteTimeModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, num_timesteps: int):\n",
    "        super(BasicDiscreteTimeModel, self).__init__()\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(embed_dim=embed_dim, max_len=num_timesteps)\n",
    "        self.score_network = ScoreNetwork(input_dim=input_dim, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        time_embed = self.positional_embeddings(t)\n",
    "        return self.score_network(x, time_embed)\n",
    "\n",
    "    \n",
    "\n",
    "class CircleDiffusionAdaptor(nn.Module):\n",
    "    def __init__(self, config, dataset, model_dir=None):\n",
    "        super(CircleDiffusionAdaptor, self).__init__()\n",
    "        # self.config = load_yaml_config(config)\n",
    "        self.config = config\n",
    "        \n",
    "        if not model_dir is None:\n",
    "            self.model_dir = model_dir\n",
    "        else:\n",
    "            self.model_dir = config['base_path']\n",
    "        \n",
    "        #if not os.path.exists(model_dir):\n",
    "        #    os.mkdir(model_dir)\n",
    "        #self.model_dir = model_dir\n",
    "        self.input_dim = config['input_dim']\n",
    "        try: \n",
    "            self.num_timesteps = config['num_timesteps']\n",
    "        except KeyError: \n",
    "            pass\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.input_idx = [idx for idx, element in enumerate(self.dataset.attributes) if element not in ['Confounder', 'Target']]\n",
    "        self.target_idx = [idx for idx, element in enumerate(self.dataset.attributes) if element == 'Target']\n",
    "        #data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "        #for idx, key in enumerate(dataset.data):\n",
    "        #    data[idx] = dataset.data[key]\n",
    "        #self.model = self.train_and_load_diffusion(model_name='diffusion.pth')\n",
    "        \n",
    "        def schedules(num_timesteps: int, type: str='linear'):\n",
    " \n",
    "            if type=='linear':\n",
    "                scale = 1000 / num_timesteps\n",
    "                min_var = scale * 1e-4\n",
    "                max_var = scale * 1e-2\n",
    "                return torch.linspace(min_var, max_var, num_timesteps, dtype=torch.float32)\n",
    "            elif type=='cosine':\n",
    "                steps = num_timesteps + 1\n",
    "                x = torch.linspace(0, num_timesteps, steps, dtype=torch.float64)\n",
    "                alphas_cumprod = torch.cos(((x / num_timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "                alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "                betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "                return torch.clip(betas, 0, 0.999)\n",
    "        \n",
    "        betas = schedules(num_timesteps=config['num_timesteps'], type=config['var_schedule'])\n",
    "\n",
    "        self.register_buffer(\"beta\", betas)\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(0))\n",
    "        \n",
    "        \n",
    "    def forward_diffusion(self, clean_x: torch.Tensor, noise: torch.tensor, timestep: torch.Tensor):\n",
    "        \n",
    "        if isinstance(timestep, int):\n",
    "            timestep = torch.tensor([timestep])\n",
    "            alpha_bar_t = self.alpha_bar[timestep].repeat(clean_x.shape[0])[:, None]\n",
    "        else:\n",
    "            alpha_bar_t = self.alpha_bar[timestep][:, None]\n",
    "        mu = torch.sqrt(alpha_bar_t)\n",
    "        std = torch.sqrt(1 - alpha_bar_t)\n",
    "        noisy_x = mu * clean_x + std * noise\n",
    "        return noisy_x\n",
    "    \n",
    "\n",
    "    def reverse_diffusion_ddpm(self, noisy_x: torch.Tensor, model: nn.Module, timestep: torch.Tensor):\n",
    "        alpha_t = self.alpha[timestep].repeat(noisy_x.shape[0])[:, None]\n",
    "        alpha_bar_t = self.alpha_bar[timestep].repeat(noisy_x.shape[0])[:, None]\n",
    "        beta_t = 1 - alpha_t\n",
    "        eps_hat = model(x=noisy_x, t=timestep)\n",
    "        posterior_mean = (1 / torch.sqrt(alpha_t)) * (noisy_x - (beta_t / torch.sqrt(1 - alpha_bar_t) * eps_hat))\n",
    "        z = torch.randn_like(noisy_x)\n",
    "        \n",
    "        if timestep > 0:\n",
    "            alpha_bar_t_minus_1 = self.alpha_bar[timestep-1].repeat(noisy_x.shape[0])[:, None]\n",
    "            sigma_t = beta_t * (1 - alpha_bar_t_minus_1) / (1 - alpha_bar_t)\n",
    "            denoised_x = posterior_mean + torch.sqrt(sigma_t)*z #* z * (timestep > 0))  # variance = beta_t\n",
    "        else:\n",
    "            denoised_x = posterior_mean\n",
    "                                           \n",
    "        return denoised_x\n",
    "    \n",
    "    def train_and_load_diffusion(self, model_name='diffusion.pt', mode=None):\n",
    "        \n",
    "        self.model_path = os.path.join(self.model_dir, model_name)\n",
    "        model = BasicDiscreteTimeModel(input_dim=self.config['input_dim'], embed_dim=self.config['embed_dim'], num_timesteps=self.config['num_timesteps'])\n",
    "        if model_name in os.listdir(self.model_dir) and not mode == \"train\":\n",
    "            model.load_state_dict(torch.load(self.model_path))\n",
    "            logging.info(f'Model found with path {self.model_path}')\n",
    "        elif model_name not in os.listdir(self.model_dir) and mode != 'train':\n",
    "            logging.info('Model not found. Please run train_and_load_diffusion method and set its argument mode=\"train\" ')\n",
    "        else:\n",
    "            logging.info(\n",
    "                f'Training model with path {self.model_path}'\n",
    "            )\n",
    "        \n",
    "        def diffusion_loss(model: nn.Module, clean_x: torch.Tensor) -> torch.Tensor:\n",
    "            t = torch.randint(self.num_timesteps, (clean_x.shape[0],))\n",
    "            eps_t = torch.randn_like(clean_x)\n",
    "            alpha_bar_t = self.alpha_bar[t][:, None]\n",
    "            x_t = self.forward_diffusion(clean_x=clean_x, noise=eps_t, timestep=t)\n",
    "            eps_hat = model(x=x_t, t=t)\n",
    "            loss_diff = nn.MSELoss(reduction='sum')(eps_hat, eps_t)\n",
    "            \n",
    "            return loss_diff\n",
    "                    \n",
    "        def run_epoch(model: nn.Module, dataloader: torch.utils.data.dataloader.DataLoader):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for x, _ in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = diffusion_loss(model, x[:, self.input_idx])\n",
    "                epoch_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            return epoch_loss / len(dataloader.dataset)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            model.train()\n",
    "            num_epochs = self.config['num_epochs']\n",
    "            dataloader = DataLoader(self.dataset, batch_size=self.config['batch_size'], shuffle=True)\n",
    "            learning_rate = 1e-4\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            losses = []\n",
    "            for i in trange(num_epochs):\n",
    "                epoch_loss = 0.0\n",
    "                for x, _ in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = diffusion_loss(model, x[:, self.input_idx])\n",
    "                    epoch_loss += loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_loss = epoch_loss / len(dataloader.dataset)\n",
    "                print(f'Epoch: {i}, train_loss: {train_loss}')\n",
    "                losses.append(train_loss.detach().numpy())\n",
    "            \n",
    "            torch.save(model.state_dict(), self.model_path) \n",
    "            \n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_ddpm(self, model: nn.Module, n_samples: int = 256, label=None):\n",
    "        \"\"\"\n",
    "        iteratively denoises pure noise to produce a list of denoised samples at each timestep\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        x_pred = []\n",
    "        x = torch.randn(n_samples, self.input_dim)\n",
    "        x_pred.append(x)\n",
    "\n",
    "        for t in reversed(range(0, self.num_timesteps)):\n",
    "                \n",
    "            x = self.reverse_diffusion_ddpm(noisy_x=x, model=model, timestep=t)\n",
    "                \n",
    "            x_pred.append(x)\n",
    "        return x_pred\n",
    "    \n",
    "    def sample_x(self, batch_size=1):\n",
    "        x = self.sample_ddpm(model=self.model, n_samples=batch_size)[-1] \n",
    "        return x\n",
    "\n",
    "    def sample_counterfactual_ddpm(self, clean_batch: torch.Tensor, model: nn.Module, classifier: nn.Module, num_noise_steps: int, target_classes: int, classifier_grad_weight: float):\n",
    "        \n",
    "        \n",
    "        classifier.eval()\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        # DEFINE BATCH SIZE AND COUNTERFACTUAL CLASS\n",
    "        bs = clean_batch.shape[0]\n",
    "\n",
    "        # COMPUTE CLEAN GRADIENTS FOR THE FIRST STEP\n",
    "        \n",
    "        classifier_criterion = lambda x: F.cross_entropy(classifier(x), target_classes)\n",
    "        clean_batch_copy = torch.nn.Parameter(clean_batch)\n",
    "        loss = classifier_criterion(clean_batch_copy)\n",
    "        loss.backward()\n",
    "        clean_grad = classifier_grad_weight * clean_batch_copy.grad.detach()\n",
    "        \n",
    "        # PERFORMING FORWARD DIFFUSION UNTIL NUM_NOISE_STEPS\n",
    "        eps_t = torch.randn_like(clean_batch)\n",
    "        next_z = self.forward_diffusion(clean_x=clean_batch, noise=eps_t, timestep=num_noise_steps)\n",
    "        counterfactuals = [] # total counterfactuals\n",
    "        counterfactuals.append(clean_batch)\n",
    "        guided_grads = []  # guided grads at the first step\n",
    "        unconditional_grads = [] # diffusion grads at the first step\n",
    "        total_series = [] # contains evolution from noisy to cleaned instance for each data point\n",
    "        for i in tqdm(range(0, num_noise_steps)[::-1]):\n",
    "            # Denoise z_t to create z_t-1 (next z)\n",
    "            alpha_i = self.alpha[i].repeat(bs)[:, None]\n",
    "            alpha_bar_i = self.alpha_bar[i].repeat(bs)[:, None]\n",
    "            sigma_i = torch.sqrt(1 - self.alpha[i])\n",
    "            eps_hat = model(next_z, i)\n",
    "\n",
    "            # Unconditional mean\n",
    "            unconditional_grad = -eps_hat / torch.sqrt(1 - alpha_bar_i)\n",
    "            z_t_mean = (next_z + unconditional_grad * (1 - alpha_i)) / torch.sqrt(alpha_i)\n",
    "\n",
    "            # Guided mean\n",
    "            z_t_mean -= sigma_i * (clean_grad / torch.sqrt(alpha_bar_i))\n",
    "\n",
    "            if i > 0:\n",
    "                next_z = z_t_mean + (sigma_i * torch.randn_like(clean_batch))\n",
    "            else:\n",
    "                next_z = z_t_mean\n",
    "\n",
    "            next_x = next_z.clone()\n",
    "            # Denoise to create a cleaned x (next x)\n",
    "            series = []\n",
    "            series.append(next_x.detach())\n",
    "            for t in range(0, i)[::-1]:\n",
    "                if i == 0:\n",
    "                    break\n",
    "                next_x = self.reverse_diffusion_ddpm(noisy_x=next_x, model=model, timestep=t)\n",
    "                series.append(next_x.detach())\n",
    "            total_series.append(series)\n",
    "            guided_grads.append(-sigma_i * clean_grad.detach() / torch.sqrt(alpha_bar_i))\n",
    "            unconditional_grads.append(unconditional_grad.detach() * (1 - alpha_i) / torch.sqrt(alpha_i) )\n",
    "            \n",
    "            \n",
    "            if i != 0:\n",
    "                counterfactuals.append(next_x.detach())\n",
    "\n",
    "            # Gradient wrt denoised image (next_x)\n",
    "            next_x_copy = torch.nn.Parameter(next_x.clone())\n",
    "            loss = classifier_criterion(next_x_copy)\n",
    "            loss.backward()\n",
    "            clean_classifier_grad = next_x_copy.grad.detach()\n",
    "            clean_grad = classifier_grad_weight * clean_classifier_grad\n",
    "            \n",
    "            #self.counterfactuals = counterfactuals\n",
    "            #self.guided_grads = guided_grads\n",
    "            #self.diffusion_grads = unconditional_grads\n",
    "            #self.pointwise_evolution = total_series \n",
    "            \n",
    "        counterfactuals = torch.stack(counterfactuals).permute(1, 0, 2) \n",
    "        guided_grads = torch.stack(guided_grads).permute(1, 0, 2) \n",
    "        unguided_grads = torch.stack(unconditional_grads).permute(1, 0, 2)\n",
    "        \n",
    "        self.counterfactuals_series = counterfactuals\n",
    "        \n",
    "        return counterfactuals, guided_grads, unguided_grads, total_series\n",
    "\n",
    "    \n",
    "    def discard_counterfactuals(self, counterfactuals, classifier, target_classes, target_confidence, minimal_counterfactuals, tolerance=0.1):\n",
    "        \n",
    "        # compute distance of current minimal_counterefactuals from radius 1.0\n",
    "        #current_counterfactual_distance_from_manifold = torch.abs((torch.pow(minimal_counterfactuals, 2).sum(dim=-1) - 1.0))\n",
    "        \n",
    "        for i in range(len(counterfactuals)):  \n",
    "\n",
    "            # compute classifier  for all the counterfactuals for each point\n",
    "            new_counterfactuals_confidence = classifier(counterfactuals[i]).softmax(dim=-1)[:, target_classes[i]]\n",
    "            \n",
    "            \n",
    "            # check if new counterfactuals satisfy the confidence constraint\n",
    "            new_confidence_satisfied = new_counterfactuals_confidence > target_confidence\n",
    "            \n",
    "            new_confidence_satisfied_indices = torch.nonzero(new_counterfactuals_confidence > target_confidence)\n",
    "            \n",
    "            \n",
    "            # \n",
    "            #new_tolerance_satisfied = torch.abs((torch.pow(counterfactuals[i], 2).sum(dim=-1) - 1.0)) < tolerance\n",
    "            #new_tolerance_satisfied_indices = torch.nonzero(torch.abs((torch.pow(counterfactuals[i], 2).sum(dim=-1) - 1.0)) < tolerance)\n",
    "            \n",
    "            # check where target confidence is reached AND if new counterfactuals are within the tolerance range\n",
    "            #indices = torch.nonzero((confidence > target_confidence) \n",
    "            #                    (torch.abs((torch.pow(counterfactuals[i], 2).sum(dim=-1) - 1.0)) < tolerance))\n",
    "            \n",
    "            \n",
    "            #new_confidence_and_tolerance_satisfied_indices = torch.nonzero(new_confidence_satisfied & new_tolerance_satisfied)\n",
    "            \n",
    "            #current_tolerance_satisfied = current_counterfactual_distance_from_manifold[i] < tolerance\n",
    "            #classifier(minimal_counterfactuals[i:i+1]).softmax(dim=-1)[0][target_classes[i]]\n",
    "            current_confidence_satisfied = classifier(minimal_counterfactuals[i:i+1]).softmax(dim=-1)[0][target_classes[i]].item() > target_confidence\n",
    "            \n",
    "            # if current counterfactual satisfies confidence and tolerance, maintain status quo \n",
    "          \n",
    "            if new_confidence_satisfied_indices.nelement() != 0:\n",
    "                print('new confidence satisfied')\n",
    "                minimal_counterfactuals[i] = counterfactuals[i][new_confidence_satisfied_indices[0].item()]\n",
    "                \n",
    "            else:\n",
    "                print('neither current nor new confidence satisfied')\n",
    "                minimal_counterfactuals[i] = counterfactuals[i][-1]\n",
    "                \n",
    "            # if new target confidence and current target confidence not satisfied\n",
    "            # but new tolerance is satisfied, then move the point closer\n",
    "            #elif (new_tolerance_satisfied_indices.nelement() !=0) & (new_confidence_satisfied_indices.nelement() == 0) and (not current_confidence_satisfied):\n",
    "            #    minimal_counterfactuals[i] = counterfactuals[i][torch.nonzero(new_tolerance_satisfied)[-1].item()]\n",
    "            \n",
    "              \n",
    "            # if current counterfactual is on the manifold but is not actually a counterfactual, \n",
    "            # replace it with new counterfactual if there exists any \n",
    "            #elif (not current_confidence_satisfied) & (new_confidence_and_tolerance_satisfied_indices.nelement() != 0):\n",
    "            #    # change this to only include the first where confidence and tolerance is satisfied\n",
    "            #    minimal_counterfactuals[i] = counterfactuals[i][new_confidence_and_tolerance_satisfied_indices[0].item()]\n",
    "                \n",
    "            #else:\n",
    "            #    continue\n",
    "            \n",
    "\n",
    "        return minimal_counterfactuals\n",
    "        \n",
    "        \n",
    "    def edit(\n",
    "        self,\n",
    "        x_in: torch.Tensor,\n",
    "        target_confidence_goal: float,\n",
    "        target_classes: torch.Tensor,\n",
    "        classifier: nn.Module\n",
    "    ) -> Tuple[list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]]:\n",
    "        \n",
    "        self.original_sample = x_in\n",
    "        #minimal_counterfactuals = torch.zeros(size=x_in.shape)\n",
    "\n",
    "        scales = self.config['grad_scales']\n",
    "        noise_steps = self.config['noise_steps_for_counterfactuals']\n",
    "        \n",
    "        minimal_counterfactuals = x_in.clone()\n",
    "        \n",
    "        for it in range(self.config['num_iterations']):\n",
    "            for steps in noise_steps:\n",
    "\n",
    "                for s in scales:\n",
    "\n",
    "                    counterfactuals, guided_grads, unguided_grads, total_series = self.sample_counterfactual_ddpm(clean_batch=minimal_counterfactuals, model=self.model, classifier=classifier, num_noise_steps=steps, target_classes=target_classes, classifier_grad_weight=s)\n",
    "\n",
    "                    minimal_counterfactuals = self.discard_counterfactuals(counterfactuals=counterfactuals, classifier=classifier, target_confidence=target_confidence_goal, target_classes=target_classes, minimal_counterfactuals=minimal_counterfactuals)\n",
    "                    self.counterfactuals = minimal_counterfactuals\n",
    "                    flip_rate = sum(classifier(minimal_counterfactuals).softmax(dim=-1).argmax(dim=-1) != classifier(x_in).softmax(dim=-1).argmax(dim=-1)) / len(x_in)\n",
    "                    self.plot_counterfactuals()\n",
    "                    plt.title(f'Noise Steps: {steps}, Gradient Scale: {s}, Flip Rate: {round(flip_rate.item(),3)}')\n",
    "                    #plt.show()\n",
    "        list_counterfactuals = [row_tensor for row_tensor in minimal_counterfactuals]\n",
    "        diff_latent = x_in - minimal_counterfactuals\n",
    "        \n",
    "        confidences = classifier(minimal_counterfactuals).softmax(dim=-1)\n",
    "        y_target_end_confidence = [confidences[i][target_classes[i]].detach() for i in range(len(minimal_counterfactuals))]\n",
    "        x_list = [row_tensor for row_tensor in x_in]\n",
    "        \n",
    "        #self.counterfactuals = minimal_counterfactuals\n",
    "        \n",
    "        return list_counterfactuals, diff_latent, y_target_end_confidence, x_list\n",
    "    \n",
    "    \n",
    "    def plot_counterfactuals(self):\n",
    "        #plt.figure(figsize=(5,5))\n",
    "        data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "        for idx, key in enumerate(dataset.data):\n",
    "            data[idx] = dataset.data[key]\n",
    "        self.data = data\n",
    "        plt.scatter(data[:,self.input_idx[0]], data[:,self.input_idx[1]], c=np.where(data[:,self.target_idx] == 0, 'lightcyan', 'lightgray')[0])\n",
    "        for i, point in enumerate(self.counterfactuals):\n",
    "            plt.scatter(self.original_sample[i, 0], self.original_sample[i, 1], color='green', label='start')\n",
    "            plt.scatter(point[0], point[1], color='red', label='end')\n",
    "            plt.arrow(\n",
    "                self.original_sample[i,0], self.original_sample[i, 1], # plot the original point plus arrow until (j+granularity)th point\n",
    "                point[0] - self.original_sample[i, 0], \n",
    "                point[1] - self.original_sample[i, 1],\n",
    "                head_width=0.05, head_length=0.05, fc='blue', ec='blue',\n",
    "\n",
    "            )\n",
    "        \n",
    "        #plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peal",
   "language": "python",
   "name": "peal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
