{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T17:22:11.828675100Z",
     "start_time": "2023-09-25T17:22:08.862917800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "# try to import peal and if not installed, add the parent directory to the path\n",
    "try:\n",
    "    import peal\n",
    "\n",
    "except ImportError:\n",
    "    # if peal not installed, but project downloaded locally\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# import basic libraries needed for sure and set the device depending on whether cuda is available or not\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set autoreload for more convinient development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from peal.global_utils import request\n",
    "# check and set that the right gpu is used\n",
    "if device == 'cuda':\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    !nvidia-smi\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    print('Currently used device: ' + str(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = request(\n",
    "        'cuda_visible_devices', default=\"0\")\n",
    "    torch.cuda.set_device(int(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    import math\n",
    "    import nvidia_smi\n",
    "    nvidia_smi.nvmlInit()\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    gigabyte_vram = info.total / math.pow(10, 9)\n",
    "    print(\"Total memory:\", gigabyte_vram)\n",
    "\n",
    "else:\n",
    "    gigabyte_vram = None\n",
    "\n",
    "    \n",
    "is_asking = request('asking', default = True)#\n",
    "unrestricted_unpoisened = request('unrestricted_unpoisened', default = False, is_asking=is_asking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T17:23:07.192439800Z",
     "start_time": "2023-09-25T17:23:04.732530900Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "from peal.data.dataset_factory import get_datasets\n",
    "from peal.data.dataset_generators import CircleDatasetGenerator\n",
    "from peal.global_utils import load_yaml_config\n",
    "import copy\n",
    "\n",
    "unpoisened_dataset_config = load_yaml_config('<PEAL_BASE>/configs/data/symbolic_circle.yaml')\n",
    "unpoisened_dataset_config.num_samples = 1088\n",
    "unpoisened_dataset_config.noise_scale = 0\n",
    "unpoisened_dataset_config.set_negative_to_zero = False\n",
    "dg = CircleDatasetGenerator(\n",
    "    dataset_name='circle',\n",
    "    num_samples=unpoisened_dataset_config.num_samples,\n",
    "    radius=unpoisened_dataset_config.radius,\n",
    "    noise_scale=unpoisened_dataset_config.noise_scale,\n",
    "    seed=unpoisened_dataset_config.seed,\n",
    ")\n",
    "\n",
    "dg.generate_dataset()\n",
    "\n",
    "student_config = load_yaml_config('<PEAL_BASE>/configs/models/symbolic_circle_classifier.yaml')\n",
    "if len(student_config.task['x_selection']):\n",
    "    unpoisened_dataset_config.input_size = [len(student_config.task['x_selection'])]\n",
    "    \n",
    "unpoisened_dataset_train, unpoisened_dataset_val, unpoisened_dataset_test = get_datasets(\n",
    "    config=unpoisened_dataset_config,\n",
    "    base_dir=dg.label_dir,\n",
    "    task_config=student_config.task,\n",
    ")\n",
    "\n",
    "# create a copy of the dataset config that will be poisened in the next steps\n",
    "poisened_dataset_config = copy.deepcopy(unpoisened_dataset_config)\n",
    "poisened_dataset_config.num_samples = int(unpoisened_dataset_config.num_samples / 2)\n",
    "\n",
    "confounder_probability = request('confounder_probability', '100')\n",
    "poisened_dataset_config.confounder_probability = float(confounder_probability) / 100\n",
    "\n",
    "# create dataset based changed data config\n",
    "poisened_dataset_train, poisened_dataset_val, poisened_dataset_test = get_datasets(\n",
    "    config=poisened_dataset_config,\n",
    "    base_dir=dg.label_dir,\n",
    "    task_config=student_config.task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "namespace(training={'train_batch_size': 100,\n                    'val_batch_size': 100,\n                    'test_batch_size': 100,\n                    'max_epochs': 12,\n                    'learning_rate': 0.0001,\n                    'optimizer': 'adam',\n                    'global_train_step': 0,\n                    'global_validation_step': 0,\n                    'epoch': -1,\n                    'steps_per_epoch': 1000,\n                    'verbosity': 1},\n          architecture={'layers': [['fc', 512, 0.25], ['fc', 512, 0.5]],\n                        'activation': 'ReLU'},\n          task={'criterions': {'ce': 1.0, 'l1': 10000.0},\n                'output_type': 'multiclass',\n                'output_size': 2,\n                'x_selection': ['x1', 'x2'],\n                'y_selection': ['Target'],\n                'selection': ['x1', 'x2']})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T17:06:26.432456600Z",
     "start_time": "2023-09-25T17:06:26.332366100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'request' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m is_train_generator \u001B[38;5;241m=\u001B[39m \u001B[43mrequest\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis_train_generator\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_train_generator:\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# if you want the generator getting trained from scratch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerators\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvariational_autoencoders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VAE\n",
      "\u001B[0;31mNameError\u001B[0m: name 'request' is not defined"
     ]
    }
   ],
   "source": [
    "is_train_generator = request('is_train_generator', True)\n",
    "if is_train_generator:\n",
    "    # if you want the generator getting trained from scratch\n",
    "    from peal.generators.variational_autoencoders import VAE\n",
    "    from peal.training.trainers import ModelTrainer\n",
    "    from peal.data.dataset_wrappers import VAEDatasetWrapper\n",
    "    generator_config = load_yaml_config('<PEAL_BASE>/configs/models/symbolic_circle_vae.yaml')\n",
    "    generator_config.data = unpoisened_dataset_train.config\n",
    "    generator_config.training['max_epochs'] = 5\n",
    "    generator = VAE(generator_config).to(device)\n",
    "\n",
    "    dataset_train = VAEDatasetWrapper(unpoisened_dataset_train)\n",
    "    dataset_val = VAEDatasetWrapper(unpoisened_dataset_val)\n",
    "\n",
    "    generator_trainer = ModelTrainer(\n",
    "        config=generator_config,\n",
    "        model=generator,\n",
    "        datasource=(dataset_train, dataset_val),\n",
    "        model_name=request(\n",
    "            'generator_model_name',\n",
    "            'artificial_symbolic_' + confounder_probability + '_generator'\n",
    "        ),\n",
    "        gigabyte_vram=gigabyte_vram\n",
    "    )\n",
    "    generator_trainer.fit()\n",
    "\n",
    "else:\n",
    "    # if you want to use loaded generator\n",
    "    generator_path = request(\n",
    "        'generator_path',\n",
    "        'peal_runs/artificial_symbolic_' + confounder_probability + '_generator/model.cpl'\n",
    "    )\n",
    "    generator = torch.load(generator_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to change value of is_train_student==True? [y/n] n\n",
      "Do you want to change value of student_model_name==artificial_symbolic_100_classifier? [y/n] n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Config: {'training': {'train_batch_size': 100, 'val_batch_size': 100, 'test_batch_size': 100, 'max_epochs': 5, 'learning_rate': 0.0001, 'optimizer': 'adam', 'global_train_step': 0, 'global_validation_step': 0, 'epoch': -1, 'iterations_per_episode': 1000, 'verbosity': 1}, 'architecture': {'activation': 'ReLU', 'neuron_numbers_encoder': [512], 'neuron_numbers_decoder': [512]}, 'task': {'criterions': {'ce': 1.0, 'l1': 10000.0}, 'output_type': 'multiclass', 'output_size': 2, 'x_selection': ['x1', 'x2'], 'y_selection': ['Target'], 'selection': ['x1', 'x2']}, 'data': {'radius': 1, 'noise_scale': 0, 'num_samples': 250, 'split': [0.8, 0.9], 'input_type': 'symbolic', 'input_size': [2], 'label_noise': 0.0, 'seed': 0, 'output_type': 'multiclass', 'output_size': 2, 'confounder_probability': 1.0, 'confounding_factors': ['Target', 'Confounder'], 'known_confounder': True, 'set_negative_to_zero': False}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: validation_0_it: 0, loss: 1.211959215652314e-06Epoch: 4, train_loss_accumulated: 1.525195102658472e-06, train_accuracy: 1.0, train_predicted_classes: tensor([0.5000, 0.5000]), train_targets: tensor([0.5000, 0.5000]), train_classes_difference: tensor([0., 0.]), validation_0_loss_accumulated: 2.582859679023386e-06, validation_0_accuracy: 1.0, validation_0_predicted_classes: tensor([0.5000, 0.5000]), validation_0_targets: tensor([0.5000, 0.5000]), validation_0_classes_difference: tensor([0., 0.]): : 5005it [03:30, 23.81it/s]                   \n"
     ]
    }
   ],
   "source": [
    "is_train_student = request('is_train_student', True)\n",
    "if is_train_student:\n",
    "    # if you want to train your own initial student model\n",
    "    from peal.architectures.downstream_models import Symbolic2VectorModel\n",
    "    from peal.training.trainers import ModelTrainer\n",
    "    student_config = load_yaml_config('<PEAL_BASE>/configs/models/symbolic_circle_classifier.yaml')\n",
    "    student_config.data = poisened_dataset_train.config\n",
    "    student_config.training['max_epochs'] = 5 \n",
    "    # create and traing student model\n",
    "    student = Symbolic2VectorModel(student_config).to(device)\n",
    "    student_trainer = ModelTrainer(\n",
    "        config=student_config,\n",
    "        model=student,\n",
    "        datasource=(poisened_dataset_train, poisened_dataset_val),\n",
    "        model_name=request(\n",
    "            'student_model_name',\n",
    "            'artificial_symbolic_' + confounder_probability + '_classifier'\n",
    "        ),\n",
    "        gigabyte_vram=gigabyte_vram\n",
    "    )\n",
    "    student_trainer.fit()\n",
    "\n",
    "else:\n",
    "    # if you want to load your initial student model\n",
    "    student_path = request(\n",
    "        'student_path',\n",
    "        'peal_runs/artificial_symbolic_' + confounder_probability + '_classifier/model.cpl'\n",
    "    )\n",
    "    student = torch.load(student_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to change value of teacher_type==train? [y/n] n\n",
      "Do you want to change value of teacher_model_name==artificial_symbolic_unpoisened_classifier? [y/n] n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Config: {'training': {'train_batch_size': 100, 'val_batch_size': 100, 'test_batch_size': 100, 'max_epochs': 5, 'learning_rate': 0.0001, 'optimizer': 'adam', 'global_train_step': 0, 'global_validation_step': 0, 'epoch': -1, 'iterations_per_episode': 1000, 'verbosity': 1}, 'architecture': {'activation': 'ReLU', 'neuron_numbers_encoder': [512], 'neuron_numbers_decoder': [512]}, 'task': {'criterions': {'ce': 1.0, 'l1': 10000.0}, 'output_type': 'multiclass', 'output_size': 2, 'x_selection': ['x1', 'x2'], 'y_selection': ['Target'], 'selection': ['x1', 'x2']}, 'data': {'radius': 1, 'noise_scale': 0, 'num_samples': 500, 'split': [0.8, 0.9], 'input_type': 'symbolic', 'input_size': [2], 'label_noise': 0.0, 'seed': 0, 'output_type': 'multiclass', 'output_size': 2, 'confounder_probability': 0.5, 'confounding_factors': ['Target', 'Confounder'], 'known_confounder': True, 'set_negative_to_zero': False}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: validation_0_it: 0, loss: 0.009123411029577255Epoch: 4, train_loss_accumulated: 0.00103426119312644, train_accuracy: 1.0, train_predicted_classes: tensor([0.5000, 0.5000]), train_targets: tensor([0.5000, 0.5000]), train_classes_difference: tensor([0., 0.]), validation_0_loss_accumulated: 0.01188803743571043, validation_0_accuracy: 1.0, validation_0_predicted_classes: tensor([0.5000, 0.5000]), validation_0_targets: tensor([0.5000, 0.5000]), validation_0_classes_difference: tensor([0., 0.]): : 5005it [02:42, 30.76it/s] \n"
     ]
    }
   ],
   "source": [
    "teacher_type = request('teacher_type', 'train') # changes this to load\n",
    "if teacher_type == 'train':\n",
    "    # if you want to train and use new model for knowledge distillation\n",
    "    from peal.architectures.downstream_models import Symbolic2VectorModel\n",
    "    from peal.training.trainers import ModelTrainer\n",
    "    teacher_config = load_yaml_config('<PEAL_BASE>/configs/models/symbolic_circle_classifier.yaml')\n",
    "    teacher_config.data = u\n",
    "    npoisened_dataset_train.config\n",
    "    teacher_config.training['max_epochs'] = 5\n",
    "\n",
    "    # create and train teacher model\n",
    "    teacher = Symbolic2VectorModel(teacher_config).to(device)\n",
    "    teacher_trainer = ModelTrainer(\n",
    "        config=teacher_config,\n",
    "        model=teacher,\n",
    "        datasource=(unpoisened_dataset_train, unpoisened_dataset_val),\n",
    "        model_name=request('teacher_model_name', 'artificial_symbolic_unpoisened_classifier'),\n",
    "        gigabyte_vram=gigabyte_vram\n",
    "    )\n",
    "    teacher_trainer.fit()\n",
    "    teacher_type = 'oracle'\n",
    "\n",
    "elif teacher_type == 'load':\n",
    "    # if you want to use existing model for knowledge distillation\n",
    "    teacher_path = request(\n",
    "        'teacher_path', 'peal_runs/artificial_symbolic_unpoisened_classifier/model.cpl')\n",
    "    teacher = torch.load(teacher_path).to(device)\n",
    "    teacher_type = 'oracle'\n",
    "\n",
    "else:\n",
    "    teacher = teacher_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "def plot_decision_boundary(model, dataset):\n",
    "    assert isinstance(dataset, peal.data.datasets.SymbolicDataset)\n",
    "    \n",
    "    data = torch.zeros([len(dataset.data),len(dataset.attributes)], dtype=torch.float16)\n",
    "    for idx, key in enumerate(dataset.data):\n",
    "        data[idx] = dataset.data[key]\n",
    "    \n",
    "    input_idx = [idx for idx, element in enumerate(dataset.attributes) if element in dataset.task_config.x_selection]\n",
    "    target_idx = [idx for idx, element in enumerate(dataset.attributes) if element in dataset.task_config.y_selection]\n",
    "    confounder_idx =  [idx for idx, element in enumerate(dataset.attributes) if element in list(set(dataset.config.confounding_factors) - set(dataset.task_config.y_selection))]\n",
    "    #if isinstance(data, torch.Tensor):\n",
    "    #    data = data.numpy()\n",
    "    #if isinstance(data, pd.DataFrame):\n",
    "    #    data = data.to_numpy()\n",
    "    #xmin, xmax = data[:,0].min()-1, data[:,0].max()+1\n",
    "    #ymin, ymax = data[:,1].min()-1, data[:,1].max()+1\n",
    "    \n",
    "    steps = 1000\n",
    "    #input_grid = []\n",
    "    #x1_span = np.linspace(-1.5, 1.5, steps)\n",
    "    #x2_span = np.linspace(-1.5, 1.5, steps)\n",
    "    #xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "    #xx1, xx2 = np.meshgrid(*[np.linspace(float(data[:, [input_idx]].min()-0.5),float(data[:, [input_idx]].max()+0.5), 1000) for idx in input_idx])\n",
    "    #grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "    #xx1, xx2 = np.meshgrid(*[np.linspace(int(data[:, 0].min())-0.5,int(data[:, 0].min())+0.5) for idx in input_idx])\n",
    "    #grid = torch.from_numpy(np.array([xx1.flatten(), xx2.flatten()]).T).to(torch.float32)\n",
    "    #model.eval()\n",
    "    #z = model(grid).to(torch.float32).detach().numpy().argmax(axis=1).reshape(xx1.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    #idx = features_idx+[class_label_idx]\n",
    "    ax.scatter(data[:,0], data[:,1], c=data[:,target_idx])\n",
    "    #ax.contour(xx1, xx2, z, levels=[0],linestyles='dashed')\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'poisened_dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpeal\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m plot_decision_boundary(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[43mpoisened_dataset_train\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'poisened_dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "import peal\n",
    "plot_decision_boundary(None, poisened_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(teacher, poisened_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to change value of cfkd_base_dir==peal_runs/artificial_symbolic_100_classifier/cfkd_oracle? [y/n] n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptor Config: {'architecture': {}, 'assumed_input_size': [3, 64, 64], 'attribution_threshold': 0.5, 'batch_size': 1, 'continuos_learning': False, 'current_iteration': 0, 'data': {'radius': 1, 'noise_scale': 0, 'num_samples': 250, 'split': [0.8, 0.9], 'input_type': 'symbolic', 'input_size': [2], 'label_noise': 0.0, 'seed': 0, 'output_type': 'multiclass', 'output_size': 2, 'confounder_probability': 1.0, 'confounding_factors': ['Target', 'Confounder'], 'known_confounder': True, 'set_negative_to_zero': False, 'has_hint': False}, 'explainer': {'explanation_style': 'counterfactual', 'gradient_steps': 51, 'img_noise_injection': 0.01, 'img_regularization': 0.0, 'l1_regularization': 1.0, 'learning_rate': 1.0, 'log_prob_regularization': 0.0, 'optimizer': 'Adam', 'use_masking': True, 'y_target_goal_confidence': 0.65}, 'fa_1sided_prime': 0.0, 'finetune_iterations': 5, 'gigabyte_vram': 40, 'max_train_samples': 2, 'max_validation_samples': 2, 'min_start_target_percentile': 0.0, 'mixing_ratio': 0.5, 'num_batches': 2, 'replace_model': True, 'replacement_strategy': 'direct', 'task': {'criterions': {'ce': 1.0, 'l1': 10000.0}, 'output_type': 'multiclass', 'output_size': 2, 'x_selection': ['x1', 'x2'], 'y_selection': ['Target'], 'selection': ['x1', 'x2']}, 'training': {'assumed_input_size': [3, 64, 64], 'base_batch_size': 50, 'epoch': -1, 'gigabyte_vram': 40, 'global_train_step': 0, 'global_validation_step': 0, 'iterations_per_episode': 1000, 'learning_rate': 0.0, 'max_epochs': 12, 'optimizer': 'adam', 'test_batch_size': 10, 'train_batch_size': 10, 'val_batch_size': 10, 'verbosity': 1}, 'use_confusion_matrix': False}\n",
      "> \u001B[0;32m/mnt/c/users/fahad/thesis/code/counterfactual_minimality/peal_private/peal/adaptors/counterfactual_knowledge_distillation.py\u001B[0m(822)\u001B[0;36mfinetune_student\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m    820 \u001B[0;31m            )\n",
      "\u001B[0m\u001B[0;32m    821 \u001B[0;31m            \u001B[0;32mimport\u001B[0m \u001B[0mpdb\u001B[0m\u001B[0;34m;\u001B[0m \u001B[0mpdb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_trace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m--> 822 \u001B[0;31m            \u001B[0mfinetune_trainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcontinue_training\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m    823 \u001B[0;31m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m    824 \u001B[0;31m        \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<peal.adaptors.counterfactual_knowledge_distillation.CounterfactualKnowledgeDistillation object at 0x7f8a24652bb0>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  dataloader.dataset.attributes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# x1', 'x2', 'Target']\n"
     ]
    }
   ],
   "source": [
    "# use counterfactual knowledge distillation to improve model\n",
    "from peal.adaptors.counterfactual_knowledge_distillation import CounterfactualKnowledgeDistillation\n",
    "cfkd = CounterfactualKnowledgeDistillation(\n",
    "    student=student,\n",
    "    datasource=(\n",
    "        poisened_dataset_train,\n",
    "        poisened_dataset_val,\n",
    "        unpoisened_dataset_test,\n",
    "    ),\n",
    "    output_size=2,\n",
    "    generator=generator,\n",
    "    teacher=teacher,\n",
    "    base_dir=request(\n",
    "        'cfkd_base_dir',\n",
    "        'peal_runs/artificial_symbolic_' + confounder_probability +\n",
    "        '_classifier/cfkd_' + teacher_type\n",
    "    ),\n",
    "    gigabyte_vram=gigabyte_vram,\n",
    "    overwrite=False,\n",
    ")\n",
    "cfkd.adaptor_config.explainer['learning_rate'] = 1.0\n",
    "cfkd.adaptor_config.explainer['gradient_steps'] = 51\n",
    "cfkd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# 1. When student is ran without generating data, confounder probability is not defined and, therefore, the logic of loading preexisting model fails (need to load data before, should be allowed to skip)\n",
    "# 2. unclear about the folder structure for cfdk\n",
    "# 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataloaders.py    \n",
    "    if isinstance(val_dataloader, torch.utils.data.dataloader.DataLoader):\n",
    "        if len(val_dataloader.dataset.data[list(val_dataloader.dataset.data.keys())[-1]]) < 1:\n",
    "            val_dataloader.dataset.data.popitem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisened_dataset_val.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpoisened_dataset_train.task_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
