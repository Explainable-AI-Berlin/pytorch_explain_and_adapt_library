# python run_cfkd.py --config "<PEAL_BASE>/configs/sce_experiments/adaptors/square1000x100_vit_b_16_sce_cfkd.yaml"
adaptor_type                         :  "CFKD"
category                             :  "adaptor"
batch_size                           :  6
tracking_level                       :  5
min_train_samples                    :  900
finetune_iterations                  :  0
max_validation_samples               :  100
continuous_learning                  :  "finetune"
use_visualization                    :  False
calculate_group_accuracies           :  True
explainer                            :  <PEAL_BASE>/configs/sce_experiments/explainers/sce_default64x64.yaml
training                             :  <PEAL_BASE>/configs/sce_experiments/training/img_finetuning.yaml
data                                 :  <PEAL_BASE>/configs/sce_experiments/data/square_poisoned100.yaml
test_data                            :  <PEAL_BASE>/configs/sce_experiments/data/square_unpoisoned.yaml
student                              :  $PEAL_RUNS/square/colora_confounding_colorb/torchvision/vit_b_16_poisoned100/model.cpl
teacher                              :  $PEAL_RUNS/square/colora_confounding_colorb/torchvision/classifier_unpoisoned/model.cpl
generator                            :  $PEAL_RUNS/square/ddpm/config.yaml
base_dir                             :  $PEAL_RUNS/square/colora_confounding_colorb/torchvision/vit_b_16_poisoned100/sce_cfkd
task:
  criterions:
    ce     : 1.0
    l2     : 100.0
  output_type             : singleclass
  output_channels         : 2
  y_selection             : [ClassA]