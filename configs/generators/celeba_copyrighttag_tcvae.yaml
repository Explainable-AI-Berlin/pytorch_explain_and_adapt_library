# python -W ignore train_generator.py --config "<PEAL_BASE>/configs/generators/celeba_copyrighttag_tcvae.yaml"
generator_type: 'DiveTCVAE'
category: 'generator'

wrapper: "tcvae"
# Hardware
ngpu: 1
amp: 0

# Optimization
batch_size: 64 #(ngpu=1 for now),
target_loss: 'val_loss'
lr_tcvae: 0.0004
max_epoch: 5000 # constraint: >=4
clip: True

# Model
model: 'biggan'
backbone: 'resnet'
channels_width: 4
z_dim: 1024
mlp_width: 10 #4
mlp_depth: 5 #2
#savedir : 'peal_runs/dive_tcvae'
base_path : '$PEAL_RUNS/celeba_copyrighttag/tcvae3'

# TCVAE
beta: 0.001 # the idea is to be able to interpolate while getting good reconstructions
tc_weight: 1 # we keep the total_correlation penalty high to encourage disentanglement
vgg_weight: 1.0
beta_annealing: True
dp_prob: 0.3

# Data
data : '<PEAL_BASE>/configs/data/copyrighttag_celeba_dive.yaml'
height: 128
width: 128

# Attacks
lr_dive: 0.01
max_iters: 20
cache_batch_size: 64
force_cache: False
stop_batch_threshold: 0.9
attribute: "Smiling"
num_explanations: 8
method: fisher spectral inv
reconstruction_weight: 10.0
lasso_weight: 1.0
diversity_weight: 1
n_samples: 10
fisher_samples: 0

#teacher: '$PEAL_RUNS/Smiling_confounding_copyrighttag_celeba/classifier_unpoisened/model.cpl'
#student: '$PEAL_RUNS/Smiling_confounding_copyrighttag_celeba/classifier_poisened100/model.cpl'

#generator_path: 'peal_runs/dive'
